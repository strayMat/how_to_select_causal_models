
@article{alaa_validating_2019,
  location = {{PMLR}},
  title    = {Validating Causal Inference Models via Influence Functions},
  abstract = {The problem of estimating causal effects of treatments from observational data falls beyond the realm of supervised learning \{—\} because counterfactual data is inaccessible, we can never observe th...},
  pages    = {191--201},
  journal  = {International Conference on Machine Learning},
  author   = {Alaa, Ahmed and Schaar, Mihaela Van Der},
  date     = {2019-05-24}
}

@article{almond_costs_2005,
  title    = {The {Costs} of {Low} {Birth} {Weight}},
  volume   = {120},
  abstract = {Low birth weight (LBW) infants experience severe health and developmental difficulties that can impose large costs on society. However, estimates of the return to LBW-prevention from cross-sectional associations may be biased by omitted variables, such as genetic factors. To address this, we compare the hospital costs, health at birth, and infant mortality rates between heavier and lighter infants from all twin pairs born in the United States. We also examine the effect of maternal smoking during pregnancy—the leading risk factor for LBW in the United States—on health among singleton births after controlling for detailed background characteristics. Both analyses imply substantially smaller effects of LBW per se than previously thought, suggesting two possibilities: 1) existing estimates overstate the true costs and consequences of LBW by at least a factor of four and by as much as a factor of twenty; or 2) different LBW-preventing interventions have different health and cost consequences, implying that policy efforts that presume a single return to reducing LBW will be suboptimal.},
  number   = {3},
  journal  = {The Quarterly Journal of Economics},
  author   = {Almond, Douglas and Chay, Kenneth Y. and Lee, David S.},
  year     = {2005}
}

@article{altman2009prognosis,
  title     = {Prognosis and prognostic research: validating a prognostic model},
  author    = {Altman, Douglas G and Vergouwe, Yvonne and Royston, Patrick and Moons, Karel GM},
  journal   = {Bmj},
  volume    = {338},
  year      = {2009},
  publisher = {British Medical Journal Publishing Group}
}

@article{athey_generalized_2019,
  title        = {Generalized random forests},
  volume       = {47},
  abstract     = {We propose generalized random forests, a method for nonparametric statistical estimation based on random forests (Breiman [Mach. Learn. 45 (2001) 5–32]) that can be used to fit any quantity of interest identified as the solution to a set of local moment equations. Following the literature on local maximum likelihood estimation, our method considers a weighted set of nearby training examples; however, instead of using classical kernel weighting functions that are prone to a strong curse of dimensionality, we use an adaptive weighting function derived from a forest designed to express heterogeneity in the specified quantity of interest. We propose a flexible, computationally efficient algorithm for growing generalized random forests, develop a large sample theory for our method showing that our estimates are consistent and asymptotically Gaussian and provide an estimator for their asymptotic variance that enables valid confidence intervals. We use our approach to develop new methods for three statistical tasks: nonparametric quantile regression, conditional average partial effect estimation and heterogeneous treatment effect estimation via instrumental variables. A software implementation, grf for R and C++, is available from {CRAN}.},
  pages        = {1148--1178},
  number       = {2},
  journal      = {Annals of Statistics},
  shortjournal = {Ann. Statist.},
  author       = {Athey, Susan and Tibshirani, Julie and Wager, Stefan},
  date         = {2019-04}
}

@article{athey_policy_2021,
  title     = {Policy Learning with Observational Data},
  abstract  = {In many areas, practitioners seek to use observational data to learn a treatment assignment policy that satisﬁes application-speciﬁc constraints, such as budget, fairness, simplicity, or other functional form constraints. For example, policies may be restricted to take the form of decision trees based on a limited set of easily observable individual characteristics. We propose a new approach to this problem motivated by the theory of semiparametrically eﬃcient estimation. Our method can be used to optimize either binary treatments or inﬁnitesimal nudges to continuous treatments, and can leverage observational data where causal eﬀects are identiﬁed using a variety of strategies, including selection on observables and instrumental variables. Given a doubly robust estimator of the causal eﬀect of assigning everyone to treatment, we develop an algorithm for choosing whom to treat, and establish strong guarantees for the asymptotic utilitarian regret of the resulting policy.},
  author    = {Athey, Susan and Wager, Stefan},
  journal   = {Econometrica},
  volume    = {89},
  number    = {1},
  pages     = {133--161},
  year      = {2021},
  publisher = {Wiley Online Library},
  keywords  = {Statistics - Machine Learning, Computer Science - Machine Learning, Economics - Econometrics, Mathematics - Statistics Theory}
}

@article{athey2016recursive,
  title     = {Recursive partitioning for heterogeneous causal effects},
  author    = {Athey, Susan and Imbens, Guido},
  journal   = {Proceedings of the National Academy of Sciences},
  volume    = {113},
  number    = {27},
  pages     = {7353--7360},
  year      = {2016},
  publisher = {National Acad Sciences}
}

@article{austin_introduction_2011,
  title    = {An {Introduction} to {Propensity} {Score} {Methods} for {Reducing} the {Effects} of {Confounding} in {Observational} {Studies}},
  abstract = {The propensity score is the probability of treatment assignment conditional on observed baseline characteristics. The propensity score allows one to design and analyze an observational (nonrandomized) study so that it mimics some of the particular characteristics of a randomized controlled trial. In particular, the propensity score is a balancing score: conditional on the propensity score, the distribution of observed baseline covariates will be similar between treated and untreated subjects. I describe 4 different propensity score methods: matching on the propensity score, stratification on the propensity score, inverse probability of treatment weighting using the propensity score, and covariate adjustment using the propensity score. I describe balance diagnostics for examining whether the propensity score model has been adequately specified. Furthermore, I discuss differences between regression-based methods and propensity score-based methods for the analysis of observational data. I describe different causal average treatment effects and their relationship with propensity score analyses.},
  number   = {3},
  journal  = {Multivariate Behavioral Research},
  author   = {Austin, Peter C.},
  month    = may,
  year     = {2011},
  pages    = {399--424}
}

@article{austin_moving_2015,
  title        = {Moving towards best practice when using inverse probability of treatment weighting ({IPTW}) using the propensity score to estimate causal treatment effects in observational studies},
  volume       = {34},
  abstract     = {The propensity score is defined as a subject's probability of treatment selection, conditional on observed baseline covariates. Weighting subjects by the inverse probability of treatment received creates a synthetic sample in which treatment assignment is independent of measured baseline covariates. Inverse probability of treatment weighting ({IPTW}) using the propensity score allows one to obtain unbiased estimates of average treatment effects. However, these estimates are only valid if there are no residual systematic differences in observed baseline characteristics between treated and control subjects in the sample weighted by the estimated inverse probability of treatment. We report on a systematic literature review, in which we found that the use of {IPTW} has increased rapidly in recent years, but that in the most recent year, a majority of studies did not formally examine whether weighting balanced measured covariates between treatment groups. We then proceed to describe a suite of quantitative and qualitative methods that allow one to assess whether measured baseline covariates are balanced between treatment groups in the weighted sample. The quantitative methods use the weighted standardized difference to compare means, prevalences, higher‐order moments, and interactions. The qualitative methods employ graphical methods to compare the distribution of continuous baseline covariates between treated and control subjects in the weighted sample. Finally, we illustrate the application of these methods in an empirical case study. We propose a formal set of balance diagnostics that contribute towards an evolving concept of ‘best practice’ when using {IPTW} to estimate causal treatment effects using observational data. © 2015 The Authors. Statistics in Medicine Published by John Wiley \& Sons Ltd.},
  pages        = {3661--3679},
  number       = {28},
  journal      = {Statistics in Medicine},
  shortjournal = {Stat Med},
  author       = {Austin, Peter C. and Stuart, Elizabeth A.},
  date         = {2015-12-10}
}

@article{austin2017estimating,
  title     = {Estimating the effect of treatment on binary outcomes using full matching on the propensity score},
  author    = {Austin, Peter C and Stuart, Elizabeth A},
  journal   = {Statistical methods in medical research},
  volume    = {26},
  number    = {6},
  pages     = {2505--2525},
  year      = {2017},
  publisher = {SAGE Publications Sage UK: London, England}
}

@article{beam2018big,
  title     = {Big data and machine learning in health care},
  author    = {Beam, Andrew L and Kohane, Isaac S},
  journal   = {Jama},
  volume    = {319},
  number    = {13},
  pages     = {1317--1318},
  year      = {2018},
  publisher = {American Medical Association}
}

@article{black1996we,
  title     = {Why we need observational studies to evaluate the effectiveness of health care},
  author    = {Black, Nick},
  journal   = {Bmj},
  volume    = {312},
  number    = {7040},
  pages     = {1215--1218},
  year      = {1996},
  publisher = {British Medical Journal Publishing Group}
}

@article{blakely2020reflection,
  title     = {Reflection on modern methods: when worlds collide--prediction, machine learning and causal inference},
  author    = {Blakely, Tony and Lynch, John and Simons, Koen and Bentley, Rebecca and Rose, Sherri},
  journal   = {International journal of epidemiology},
  volume    = {49},
  number    = {6},
  pages     = {2058--2064},
  year      = {2020},
  publisher = {Oxford University Press}
}

@article{bouthillier_accounting_2021,
  title   = {Accounting for Variance in Machine Learning Benchmarks},
  volume  = {3},
  pages   = {747--769},
  journal = {Proceedings of Machine Learning and Systems},
  author  = {Bouthillier, Xavier and Delaunay, Pierre and Bronzi, Mirko and Trofimov, Assya and Nichyporuk, Brennan and Szeto, Justin and Mohammadi Sepahvand, Nazanin and Raff, Edward and Madan, Kanika and Voleti, Vikram and Ebrahimi Kahou, Samira and Michalski, Vincent and Arbel, Tal and Pal, Chris and Varoquaux, Gael and Vincent, Pascal},
  date    = {2021-03-15}
}

@article{causalevaluations,
  title   = {An Evaluation Toolkit to Guide Model Selection and Cohort Definition in Causal Inference},
  author  = {Shimoni, Yishai and Karavani, Ehud and Ravid, Sivan and Bak, Peter and Ng, Tan Hung and Alford, Sharon Hensley and Meade, Denise and Goldschmidt, Yaara},
  journal = {arXiv preprint arXiv:1906.00442},
  year    = {2019}
}

@article{charlson_new_1987,
  title        = {A new method of classifying prognostic comorbidity in longitudinal studies: Development and validation},
  volume       = {40},
  shorttitle   = {A new method of classifying prognostic comorbidity in longitudinal studies},
  pages        = {373--383},
  number       = {5},
  journal      = {Journal of Chronic Diseases},
  shortjournal = {Journal of Chronic Diseases},
  author       = {Charlson, Mary E. and Pompei, Peter and Ales, Kathy L. and {MacKenzie}, C.Ronald},
  date         = {1987-01}
}


@article{chernozhukov_double_2018,
  title   = {Double/Debiased Machine Learning for Treatment and Structural Parameters},
  pages   = {71},
  journal = {The Econometrics Journal},
  author  = {Chernozhukov, Victor and Chetverikov, Denis and Demirer, Mert and Duflo, Esther and Hansen, Christian and Newey, Whitney and Robins, James},
  date    = {2018}
}

@article{colnet2023risk,
  title   = {Risk ratio, odds ratio, risk difference... Which causal measure is easier to generalize?},
  author  = {Colnet, B{\'e}n{\'e}dicte and Josse, Julie and Varoquaux, Ga{\"e}l and Scornet, Erwan},
  journal = {arXiv preprint arXiv:2303.16008},
  year    = {2023}
}

@article{crump_dealing_2009,
  title        = {Dealing with limited overlap in estimation of average treatment effects},
  volume       = {96},
  abstract     = {Estimation of average treatment effects under unconfounded or ignorable treatment assignment is often hampered by lack of overlap in the covariate distributions between treatment groups. This lack of overlap can lead to imprecise estimates, and can make commonly used estimators sensitive to the choice of specification. In such cases researchers have often used ad hoc methods for trimming the sample. We develop a systematic approach to addressing lack of overlap. We characterize optimal subsamples for which the average treatment effect can be estimated most precisely. Under some conditions, the optimal selection rules depend solely on the propensity score. For a wide range of distributions, a good approximation to the optimal rule is provided by the simple rule of thumb to discard all units with estimated propensity scores outside the range [0.1,0.9].},
  pages        = {187--199},
  number       = {1},
  journal      = {Biometrika},
  shortjournal = {Biometrika},
  author       = {Crump, Richard K. and Hotz, V. Joseph and Imbens, Guido W. and Mitnik, Oscar A.},
  date         = {2009-03-01}
}

@article{curth_really_2021,
  title    = {Really Doing Great at Estimating {CATE}? A Critical Look at {ML} Benchmarking Practices in Treatment Effect Estimation},
  abstract = {The machine learning ({ML}) toolbox for estimation of heterogeneous treatment effects from observational data is expanding rapidly, yet many of its algorithms have been evaluated only on a very limited set of semi-synthetic benchmark datasets. In this paper, we investigate current benchmarking practices for {ML}-based conditional average treatment effect ({CATE}) estimators, with special focus on empirical evaluation based on the popular semi-synthetic {IHDP} benchmark. We identify problems with current practice and highlight that semi-synthetic benchmark datasets, which (unlike real-world benchmarks used elsewhere in {ML}) do not necessarily reﬂect properties of real data, can systematically favor some algorithms over others – a fact that is rarely acknowledged but of immense relevance for interpretation of empirical results. Further, we argue that current evaluation metrics evaluate performance only for a small subset of possible use cases of {CATE} estimators, and discuss alternative metrics relevant for applications in personalized medicine. Additionally, we discuss alternatives for current benchmark datasets, and implications of our ﬁndings for benchmarking in {CATE} estimation.},
  pages    = {14},
  journal  = {Neurips Process 2021},
  author   = {Curth, Alicia and Svensson, David and Weatherall, James},
  date     = {2021}
}

@article{damour_overlap_2020,
  title     = {Overlap in observational studies with high-dimensional covariates},
  author    = {D’Amour, Alexander and Ding, Peng and Feller, Avi and Lei, Lihua and Sekhon, Jasjeet},
  journal   = {Journal of Econometrics},
  volume    = {221},
  number    = {2},
  pages     = {644--654},
  year      = {2021},
  publisher = {Elsevier}
}

@inbook{daniel2018double,
  author    = {Daniel, Rhian M.},
  publisher = {John Wiley \& Sons, Ltd},
  title     = {Double Robustness},
  booktitle = {Wiley StatsRef: Statistics Reference Online},
  pages     = {1-14},
  year      = {2018},
  keywords  = {causal inference, convergence, data-adaptive estimation, inverse probability weighting, local efficiency, missing data, nuisance functionals, outcome regression}
}

@article{desai2020comparison,
  title     = {Comparison of machine learning methods with traditional models for use of administrative claims with electronic medical records to predict heart failure outcomes},
  author    = {Desai, Rishi J and Wang, Shirley V and Vaduganathan, Muthiah and Evers, Thomas and Schneeweiss, Sebastian},
  journal   = {JAMA network open},
  volume    = {3},
  number    = {1},
  pages     = {e1918962--e1918962},
  year      = {2020},
  publisher = {American Medical Association}
}

@article{dorie_automated_2019,
  title      = {Automated versus do-it-yourself methods for causal inference: Lessons learned from a data analysis competition},
  shorttitle = {Automated versus do-it-yourself methods for causal inference},
  abstract   = {Statisticians have made great progress in creating methods that reduce our reliance on parametric assumptions. However this explosion in research has resulted in a breadth of inferential strategies that both create opportunities for more reliable inference as well as complicate the choices that an applied researcher has to make and defend. Relatedly, researchers advocating for new methods typically compare their method to at best 2 or 3 other causal inference strategies and test using simulations that may or may not be designed to equally tease out flaws in all the competing methods. The causal inference data analysis challenge, "Is Your {SATT} Where It's At?", launched as part of the 2016 Atlantic Causal Inference Conference, sought to make progress with respect to both of these issues. The researchers creating the data testing grounds were distinct from the researchers submitting methods whose efficacy would be evaluated. Results from 30 competitors across the two versions of the competition (black box algorithms and do-it-yourself analyses) are presented along with post-hoc analyses that reveal information about the characteristics of causal inference strategies and settings that affect performance. The most consistent conclusion was that methods that flexibly model the response surface perform better overall than methods that fail to do so. Finally new methods are proposed that combine features of several of the top-performing submitted methods.},
  author     = {Dorie, Vincent and Hill, Jennifer and Shalit, Uri and Scott, Marc and Cervone, Dan},
  journal    = {Statistical Science},
  volume     = {34},
  number     = {1},
  pages      = {43--68},
  year       = {2019},
  keywords   = {Statistics - Machine Learning, Statistics - Methodology}
}

@article{dorie_rejoinder_2019,
  title      = {Rejoinder: Response to Discussions and a Look Ahead},
  volume     = {34},
  shorttitle = {Rejoinder},
  abstract   = {Response to discussion of Dorie (2017), in which the authors of that piece express their gratitude to the discussants, rebut some specific criticisms, and argue that the limitations of the 2016 Atlantic Causal Inference Competition represent an exciting opportunity for future competitions in a similar mold.},
  pages      = {94--99},
  number     = {1},
  journal    = {Statistical Science},
  author     = {Dorie, Vincent and Hill, Jennifer and Shalit, Uri and Scott, Marc and Cervone, Dan},
  date       = {2019-02},
  keywords   = {machine learning, Causal inference, automated algorithms, competition, evaluation}
}

@article{dudley2011exploiting,
  title     = {Exploiting drug--disease relationships for computational drug repositioning},
  author    = {Dudley, Joel T and Deshpande, Tarangini and Butte, Atul J},
  journal   = {Briefings in bioinformatics},
  volume    = {12},
  number    = {4},
  pages     = {303--311},
  year      = {2011},
  publisher = {Oxford University Press}
}


@misc{econml,
  author       = {Battocchi, Keith and Dillon, Eleanor and Hei, Maggie and Lewis, Greg and  Oka, Paul and Oprescu, Miruna and Syrgkanis, Vasilis},
  title        = {{EconML}: {A Python Package for ML-Based Heterogeneous Treatment Effects Estimation}},
  howpublished = {https://github.com/microsoft/EconML}
}


@article{fontana2019can,
  title     = {Can machine learning algorithms predict which patients will achieve minimally clinically important differences from total joint arthroplasty?},
  author    = {Fontana, Mark Alan and Lyman, Stephen and Sarker, Gourab K and Padgett, Douglas E and MacLean, Catherine H},
  journal   = {Clinical orthopaedics and related research},
  volume    = {477},
  number    = {6},
  pages     = {1267},
  year      = {2019},
  publisher = {Association of Bone and Joint Surgeons}
}


@article{funk2011doubly,
  title     = {Doubly robust estimation of causal effects},
  author    = {Funk, Michele Jonsson and Westreich, Daniel and Wiesen, Chris and St{\"u}rmer, Til and Brookhart, M Alan and Davidian, Marie},
  journal   = {American journal of epidemiology},
  volume    = {173},
  number    = {7},
  pages     = {761--767},
  year      = {2011},
  publisher = {Oxford University Press}
}


@article{gao_assessment_2021,
  title    = {Assessment of heterogeneous treatment effect estimation accuracy via matching},
  abstract = {We study the assessment of the accuracy of heterogeneous treatment effect (HTE) estimation, where the HTE is not directly observable so standard computation of prediction errors is not applicable. To tackle the difficulty, we propose an assessment approach by constructing pseudo-observations of the HTE based on matching. Our contributions are three-fold: first, we introduce a novel matching distance derived from proximity scores in random forests; second, we formulate the matching problem as an average minimum-cost flow problem and provide an efficient algorithm; third, we propose a match-then-split principle for the assessment with cross-validation. We demonstrate the efficacy of the assessment approach using simulations and a real dataset.},
  number   = {17},
  journal  = {Statistics in Medicine},
  author   = {Gao, Zijun and Hastie, Trevor and Tibshirani, Robert},
  year     = {2021}
}

@article{gretton2012kernel,
  title   = {A kernel two-sample test},
  volume  = {13},
  pages   = {723--773},
  number  = {1},
  journal = {The Journal of Machine Learning Research},
  author  = {Gretton, Arthur and Borgwardt, Karsten M and Rasch, Malte J and Schölkopf, Bernhard and Smola, Alexander},
  date    = {2012}
}

@article{grose_use_2020,
  title        = {Use of Propensity Score Methodology in Contemporary High-Impact Surgical Literature},
  volume       = {230},
  pages        = {101--112.e2},
  number       = {1},
  journal      = {Journal of the American College of Surgeons},
  shortjournal = {Journal of the American College of Surgeons},
  author       = {Grose, Elysia and Wilson, Samuel and Barkun, Jeffrey and Bertens, Kimberly and Martel, Guillaume and Balaa, Fady and Khalil, Jad Abou},
  date         = {2020-01-01}
}


@article{gutierrez_causal_2016,
  title        = {Causal Inference and Uplift Modeling A review of the literature},
  series       = {Proceedings of Machine Learning Research},
  abstract     = {Uplift modeling refers to the set of techniques used to model the incremental impact of an action or treatment on a customer outcome. Uplift modeling is therefore both a Causal Inference problem and a Machine Learning one. The literature on uplift is split into 3 main approaches–the Two-Model approach, the Class Transformation approach and modeling uplift directly. Unfortunately, in the absence of a common framework of causal inference and notation, it can be quite di cult to assess those three methods. In this paper, we use the Rubin (1974) model of causal inference and its modern “econometrics” notation to provide a clear comparison of the three approaches and generalize one of them. To our knowledge, this is the ﬁrst paper that provides a uniﬁed review of the uplift literature. Moreover, our paper contributes to the literature by showing that, in the limit, minimizing the Mean Square Error ({MSE}) formula with respect to a causal e↵ect estimator is equivalent to minimizing the {MSE} in which the unobserved treatment e↵ect is replaced by a modiﬁed target variable. Finally, we hope that our paper will be of use to researchers interested in applying Machine Learning techniques to causal inference problems in a business context as well as in other ﬁelds: medicine, sociology or economics.},
  pages        = {14},
  number       = {67},
  journal      = {Proceedings of The 3rd International Conference on Predictive Applications and {APIs}},
  shortjournal = {{PMLR}},
  author       = {Gutierrez, Pierre and Gerardy, Jean-Yves},
  date         = {2016}
}



@article{hendriksen2013diagnostic,
  title     = {Diagnostic and prognostic prediction models},
  author    = {Hendriksen, Janneke MT and Geersing, Geert-Jan and Moons, Karel GM and de Groot, Joris AH},
  journal   = {Journal of Thrombosis and Haemostasis},
  volume    = {11},
  pages     = {129--141},
  year      = {2013},
  publisher = {Wiley Online Library}
}


@book{hernan_causal_2020,
  title     = {Causal Inference: What If.},
  abstract  = {Jamie Robins and I have written a book that provides a cohesive presentation of concepts of, and methods for, causal inference. Much of this material is currently scattered across journals in sever…},
  author    = {Hernán, , {MA} and Robins, {JM}},
  publisher = {CRC Boca Raton, FL},
  date      = {2020}
}

@article{hernan_methods_2021,
  title   = {Methods of Public Health Research — Strengthening Causal Inference from Observational Data},
  volume  = {385},
  pages   = {1345--1348},
  number  = {15},
  journal = {New England Journal of Medicine},
  author  = {Hernán, Miguel A.},
  date    = {2021-10-07}
}

@article{hill_bayesian_2011,
  title    = {Bayesian Nonparametric Modeling for Causal Inference},
  abstract = {Researchers have long struggled to identify causal effects in nonexperimental settings. Many recently proposed strategies assume ignorability of the treatment assignment mechanism and require fitting two models—one for the assignment mechanism and one for the response surface. This article proposes a strategy that instead focuses on very flexibly modeling just the response surface using a Bayesian nonparametric modeling procedure, Bayesian Additive Regression Trees ({BART}). {BART} has several advantages: it is far simpler to use than many recent competitors, requires less guesswork in model fitting, handles a large number of predictors, yields coherent uncertainty intervals, and fluidly handles continuous treatment variables and missing data for the outcome variable. {BART} also naturally identifies heterogeneous treatment effects. {BART} produces more accurate estimates of average treatment effects compared to propensity score matching, propensity-weighted estimators, and regression adjustment in the nonlinear simulation situations examined. Further, it is highly competitive in linear settings with the “correct” model, linear regression. Supplemental materials including code and data to replicate simulations and examples from the article as well as methods for population inference are available online.},
  pages    = {217--240},
  number   = {1},
  journal  = {Journal of Computational and Graphical Statistics},
  author   = {Hill, Jennifer L.},
  date     = {2011-01-01},
  keywords = {Causal inference, Bayesian, Nonparametrics}
}


@article{hoogland2021tutorial,
  title     = {A tutorial on individualized treatment effect prediction from randomized trials with a binary endpoint},
  author    = {Hoogland, Jeroen and IntHout, Joanna and Belias, Michail and Rovers, Maroeska M and Riley, Richard D and E. Harrell Jr, Frank and Moons, Karel GM and Debray, Thomas PA and Reitsma, Johannes B},
  journal   = {Statistics in medicine},
  volume    = {40},
  number    = {26},
  pages     = {5961--5981},
  year      = {2021},
  publisher = {Wiley Online Library}
}


@article{horng2017creating,
  title     = {Creating an automated trigger for sepsis clinical decision support at emergency department triage using machine learning},
  author    = {Horng, Steven and Sontag, David A and Halpern, Yoni and Jernite, Yacine and Shapiro, Nathan I and Nathanson, Larry A},
  journal   = {PloS one},
  volume    = {12},
  number    = {4},
  pages     = {e0174708},
  year      = {2017},
  publisher = {Public Library of Science San Francisco, CA USA}
}

@article{howe_splines_2011,
  title        = {Splines for trend analysis and continuous confounder control},
  volume       = {22},
  pages        = {874--875},
  number       = {6},
  journaltitle = {Epidemiology (Cambridge, Mass.)},
  shortjournal = {Epidemiology},
  author       = {Howe, Chanelle J. and Cole, Stephen R. and Westreich, Daniel J. and Greenland, Sander and Napravnik, Sonia and Eron, Joseph J.},
  date         = {2011-11}
}

@article{hurle2013computational,
  title     = {Computational drug repositioning: from data to therapeutics},
  author    = {Hurle, Mark R and Yang, Lun and Xie, Qing and Rajpal, Deepak K and Sanseau, Philippe and Agarwal, Pankaj},
  journal   = {Clinical Pharmacology \& Therapeutics},
  volume    = {93},
  number    = {4},
  pages     = {335--341},
  year      = {2013},
  publisher = {Wiley Online Library}
}


@book{imbens_causal_2015,
  title     = {Causal inference in statistics, social, and biomedical sciences},
  publisher = {Cambridge University Press},
  author    = {Imbens, Guido W. and Rubin, Donald B.},
  date      = {2015}
}


@article{ionides_truncated_2008,
  title        = {Truncated Importance Sampling},
  volume       = {17},
  pages        = {295--311},
  number       = {2},
  journal      = {Journal of Computational and Graphical Statistics},
  shortjournal = {Journal of Computational and Graphical Statistics},
  author       = {Ionides, Edward L},
  date         = {2008-06}
}

@article{jesson_identifying_2020,
  title    = {Identifying Causal-Effect Inference Failure with Uncertainty-Aware Models},
  abstract = {Recommending the best course of action for an individual is a major application of individual-level causal effect estimation. This application is often needed in safety-critical domains such as healthcare, where estimating and communicating uncertainty to decision-makers is crucial. We introduce a practical approach for integrating uncertainty estimation into a class of state-of-the-art neural network methods used for individual-level causal estimates. We show that our methods enable us to deal gracefully with situations of "no-overlap", common in high-dimensional data, where standard applications of causal effect approaches fail. Further, our methods allow us to handle covariate shift, where test distribution differs to train distribution, common when systems are deployed in practice. We show that when such a covariate shift occurs, correctly modeling uncertainty can keep us from giving overconfident and potentially harmful recommendations. We demonstrate our methodology with a range of state-of-the-art models. Under both covariate shift and lack of overlap, our uncertainty-equipped methods can alert decisions makers when predictions are not to be trusted while outperforming their uncertainty-oblivious counterparts.},
  author   = {Jesson, Andrew and Mindermann, Sören and Shalit, Uri and Gal, Yarin},
  journal  = {Advances in Neural Information Processing Systems},
  volume   = {33},
  pages    = {11637--11649},
  date     = {2020-10-22}
}

@article{johansson2022generalization,
  title     = {Generalization bounds and representation learning for estimation of potential outcomes and causal effects},
  author    = {Johansson, Fredrik D and Shalit, Uri and Kallus, Nathan and Sontag, David},
  journal   = {The Journal of Machine Learning Research},
  volume    = {23},
  number    = {1},
  pages     = {7489--7538},
  year      = {2022},
  publisher = {JMLRORG}
}


@article{kendall_new_1938,
  title   = {A new measure of rank correlation},
  volume  = {30},
  number  = {1-2},
  journal = {Biometrika},
  author  = {Kendall, M. G.},
  month   = jun,
  year    = {1938},
  pages   = {81--93}
}

@article{kennedy2020optimal,
  title   = {Optimal doubly robust estimation of heterogeneous causal effects},
  author  = {Kennedy, Edward H},
  journal = {arXiv preprint arXiv:2004.14497},
  year    = {2020}
}
@article{khojaste2022deep,
  title     = {Deep learning for Alzheimer's disease diagnosis: A survey},
  author    = {Khojaste-Sarakhsi, M and Haghighi, Seyedhamidreza Shahabi and Ghomi, SMT Fatemi and Marchiori, Elena},
  journal   = {Artificial Intelligence in Medicine},
  pages     = {102332},
  year      = {2022},
  publisher = {Elsevier}
}
@article{kunzel_metalearners_2019,
  title        = {Metalearners for estimating heterogeneous treatment effects using machine learning},
  volume       = {116},
  rights       = {Copyright © 2019 the Author(s). Published by {PNAS}.. https://creativecommons.org/licenses/by-nc-nd/4.0/This open access article is distributed under Creative Commons Attribution-{NonCommercial}-{NoDerivatives} License 4.0 ({CC} {BY}-{NC}-{ND}).},
  abstract     = {There is growing interest in estimating and analyzing heterogeneous treatment effects in experimental and observational studies. We describe a number of metaalgorithms that can take advantage of any supervised learning or regression method in machine learning and statistics to estimate the conditional average treatment effect ({CATE}) function. Metaalgorithms build on base algorithms—such as random forests ({RFs}), Bayesian additive regression trees ({BARTs}), or neural networks—to estimate the {CATE}, a function that the base algorithms are not designed to estimate directly. We introduce a metaalgorithm, the X-learner, that is provably efficient when the number of units in one treatment group is much larger than in the other and can exploit structural properties of the {CATE} function. For example, if the {CATE} function is linear and the response functions in treatment and control are Lipschitz-continuous, the X-learner can still achieve the parametric rate under regularity conditions. We then introduce versions of the X-learner that use {RF} and {BART} as base learners. In extensive simulation studies, the X-learner performs favorably, although none of the metalearners is uniformly the best. In two persuasion field experiments from political science, we demonstrate how our X-learner can be used to target treatment regimes and to shed light on underlying mechanisms. A software package is provided that implements our methods.},
  pages        = {4156--4165},
  number       = {10},
  journal      = {Proceedings of the National Academy of Sciences},
  shortjournal = {{PNAS}},
  author       = {Künzel, Sören R. and Sekhon, Jasjeet S. and Bickel, Peter J. and Yu, Bin},
  date         = {2019-03-05},
  keywords     = {observational studies, heterogeneous treatment effects, conditional average treatment effect, minimax optimality, randomized controlled trials}
}

@article{laan_super_2007,
  title        = {Super Learner},
  volume       = {6},
  abstract     = {When trying to learn a model for the prediction of an outcome given a set of covariates, a statistician has many estimation procedures in their toolbox. A few examples of these candidate learners are: least squares, least angle regression, random forests, and spline regression. Previous articles (van der Laan and Dudoit (2003); van der Laan et al. (2006); Sinisi et al. (2007)) theoretically validated the use of cross validation to select an optimal learner among many candidate learners. Motivated by this use of cross validation, we propose a new prediction method for creating a weighted combination of many candidate learners to build the super learner. This article proposes a fast algorithm for constructing a super learner in prediction which uses V-fold cross-validation to select weights to combine an initial set of candidate learners. In addition, this paper contains a practical demonstration of the adaptivity of this so called super learner to various true data generating distributions. This approach for construction of a super learner generalizes to any parameter which can be defined as a minimizer of a loss function.},
  journaltitle = {Statistical Applications in Genetics and Molecular Biology},
  author       = {Laan, Mark J. van der and Polley, Eric C. and Hubbard, Alan E.},
  date         = {2007-09-16}
}

@book{laan_targeted_2011,
  title    = {Targeted Learning},
  series   = {Springer Series in Statistics},
  abstract = {Establishes causal inference methodology that incorporates the benefits of machine learning with statistical inference
              
              Presentation combines accessibility with the method's rigorous grounding in statistical theory
              
              Demonstrates targeted learning in epidemiological, medical, and genomic experimental and observational studies that include informative dropout, missingness, time-dependent confounding, and case-control sampling},
  author   = {Laan, Mark J. van der and Rose, Sherri},
  date     = {2011}
}

@article{lamont2018identification,
  title     = {Identification of predicted individual treatment effects in randomized clinical trials},
  author    = {Lamont, Andrea and Lyons, Michael D and Jaki, Thomas and Stuart, Elizabeth and Feaster, Daniel J and Tharmaratnam, Kukatharmini and Oberski, Daniel and Ishwaran, Hemant and Wilson, Dawn K and Van Horn, M Lee},
  journal   = {Statistical methods in medical research},
  volume    = {27},
  number    = {1},
  pages     = {142--157},
  year      = {2018},
  publisher = {SAGE Publications Sage UK: London, England}
}

@article{loiseau_external_2022,
  title      = {External control arm analysis: an evaluation of propensity score approaches, {G}-computation, and doubly debiased machine learning},
  volume     = {22},
  shorttitle = {External control arm analysis},
  abstract   = {An external control arm is a cohort of control patients that are collected from data external to a single-arm trial. To provide an unbiased estimation of efficacy, the clinical profiles of patients from single and external arms should be aligned, typically using propensity score approaches. There are alternative approaches to infer efficacy based on comparisons between outcomes of single-arm patients and machine-learning predictions of control patient outcomes. These methods include G-computation and Doubly Debiased Machine Learning (DDML) and their evaluation for External Control Arms (ECA) analysis is insufficient.},
  journal    = {BMC Medical Research Methodology},
  author     = {Loiseau, Nicolas and Trichelair, Paul and He, Maxime and Andreux, Mathieu and Zaslavskiy, Mikhail and Wainrib, Gilles and Blum, Michael G. B.},
  year       = {2022}
}

@article{louizos_causal_2017,
  title    = {Causal Effect Inference with Deep Latent-Variable Models},
  abstract = {Learning individual-level causal effects from observational data, such as inferring the most effective medication for a speciﬁc patient, is a problem of growing importance for policy makers. The most important aspect of inferring causal effects from observational data is the handling of confounders, factors that affect both an intervention and its outcome. A carefully designed observational study attempts to measure all important confounders. However, even if one does not have direct access to all confounders, there may exist noisy and uncertain measurement of proxies for confounders. We build on recent advances in latent variable modeling to simultaneously estimate the unknown latent space summarizing the confounders and the causal effect. Our method is based on Variational Autoencoders ({VAE}) which follow the causal structure of inference with proxies. We show our method is signiﬁcantly more robust than existing methods, and matches the state-of-the-art on previous benchmarks focused on individual treatment effects.},
  journal  = {Advances in neural information processing systems},
  author   = {Louizos, Christos and Shalit, Uri and Mooij, Joris and Sontag, David and Zemel, Richard and Welling, Max},
  date     = {2017-11-06},
  keywords = {Statistics - Machine Learning, Computer Science - Machine Learning}
}

@article{macdorman_infant_1998,
  title    = {Infant mortality statistics from the linked birth/infant death data set--1995 period data},
  volume   = {46},
  abstract = {OBJECTIVES: This report presents infant mortality statistics from the linked birth/infant death data set (linked file)-1995 period data by a variety of maternal and infant characteristics. Trends in birthweight-specific infant mortality rates from 1985-95 are also discussed.
              METHODS: Descriptive tabulations of data from the linked 
              RESULTS: In general, mortality rates were lowest for infants born to Asian and Pacific Islander mothers, followed by white, American Indian, and black mothers. Rates for infants of Hispanic origin mothers were slightly lower than or comparable to those for infants of white mothers, except for infants of Puerto Rican mothers who had higher infant mortality rates. Infant mortality rates were higher for those infants whose mothers began prenatal care after the first trimester of pregnancy, were teenagers or 40 years of age or older, did not complete high school, were unmarried, or smoked during pregnancy. Infant mortality was also higher for male infants, multiple births, and infants born preterm or at low birthweight. In 1995, 63 percent of all infant deaths occurred to the 7.3 percent of infants born at low birthweight. From 1985-95, birthweight-specific infant mortality rates declined most rapidly for infants weighing 750-1,499 grams at birth. The leading causes of infant death varied considerably by race and Hispanic origin. For infants of black mothers, Disorders related to short gestation and unspecified low birthweight was the leading cause of infant death, with an infant mortality rate 4.5 times higher than that for infants of white mothers. For infants of American Indian mothers, rates for Sudden infant death syndrome were 2.9 times and for Accidents and adverse effects 3.6 times higher than those for infants of white mothers. For infants of Hispanic mothers, mortality rates from Sudden infant death syndrome were one-third lower than those for infants of white mothers.},
  language = {eng},
  number   = {6 Suppl 2},
  journal  = {Monthly Vital Statistics Report},
  author   = {MacDorman, M. F. and Atkinson, J. O.},
  month    = feb,
  year     = {1998},
  keywords = {Adolescent, Adult, Cause of Death, Ethnicity, Female, Humans, Infant, Infant Mortality, Infant, Newborn, Male, Pregnancy, United States},
  pages    = {1--22}
}

@article{minderer_revisiting_2021,
  title    = {Revisiting the {Calibration} of {Modern} {Neural} {Networks}},
  abstract = {Accurate estimation of predictive uncertainty (model calibration) is essential for the safe application of neural networks. Many instances of miscalibration in modern neural networks have been reported, suggesting a trend that newer, more accurate models produce poorly calibrated predictions. Here, we revisit this question for recent state-of-the-art image classification models. We systematically relate model calibration and accuracy, and find that the most recent models, notably those not using convolutions, are among the best calibrated. Trends observed in prior model generations, such as decay of calibration with distribution shift or model size, are less pronounced in recent architectures. We also show that model size and amount of pretraining do not fully explain these differences, suggesting that architecture is a major determinant of calibration properties.},
  author   = {Minderer, Matthias and Djolonga, Josip and Romijnders, Rob and Hubis, Frances and Zhai, Xiaohua and Houlsby, Neil and Tran, Dustin and Lucic, Mario},
  journal  = {Advances in Neural Information Processing Systems},
  volume   = {34},
  pages    = {15682--15694},
  year     = {2021}
}

@article{monshi2020deep,
  title     = {Deep learning in generating radiology reports: A survey},
  author    = {Monshi, Maram Mahmoud A and Poon, Josiah and Chung, Vera},
  journal   = {Artificial Intelligence in Medicine},
  volume    = {106},
  pages     = {101878},
  year      = {2020},
  publisher = {Elsevier}
}
@@article{mooney2018bigdata,
  title     = {Big data in public health: terminology, machine learning, and privacy},
  author    = {Mooney, Stephen J and Pejaver, Vikas},
  journal   = {Annual review of public health},
  volume    = {39},
  pages     = {95},
  year      = {2018},
  publisher = {NIH Public Access}
}


@article{naimi2021challenges,
  title   = {Challenges in Obtaining Valid Causal Effect Estimates with Machine Learning Algorithms},
  author  = {Naimi, Ashley I and Mishler, Alan E and Kennedy, Edward H},
  journal = {American Journal of Epidemiology},
  year    = {2021}
}

@article{naimi2023defining,
  title   = {Defining and Identifying Average Treatment Effects},
  author  = {Naimi, Ashley I and Whitcomb, Brian W},
  journal = {American Journal of Epidemiology},
  year    = {2023}
}

@article{nassif2022breast,
  title     = {Breast cancer detection using artificial intelligence techniques: A systematic literature review},
  author    = {Nassif, Ali Bou and Talib, Manar Abu and Nasir, Qassim and Afadar, Yaman and Elgendy, Omar},
  journal   = {Artificial Intelligence in Medicine},
  pages     = {102276},
  year      = {2022},
  publisher = {Elsevier}
}

@inproceedings{niculescu-mizil_predicting_2005,
  title     = {Predicting good probabilities with supervised learning},
  abstract  = {We examine the relationship between the predictions made by different learning algorithms and true posterior probabilities. We show that maximum margin methods such as boosted trees and boosted stumps push probability mass away from 0 and 1 yielding a characteristic sigmoid shaped distortion in the predicted probabilities. Models such as Naive Bayes, which make unrealistic independence assumptions, push probabilities toward 0 and 1. Other models such as neural nets and bagged trees do not have these biases and predict well calibrated probabilities. We experiment with two ways of correcting the biased probabilities predicted by some learning methods: Platt Scaling and Isotonic Regression. We qualitatively examine what kinds of distortions these calibration methods are suitable for and quantitatively examine how much data they need to be effective. The empirical results show that after calibration boosted trees, random forests, and SVMs predict the best probabilities.},
  language  = {en},
  booktitle = {Proceedings of the 22nd international conference on {Machine} learning  - {ICML} '05},
  publisher = {ACM Press},
  author    = {Niculescu-Mizil, Alexandru and Caruana, Rich},
  year      = {2005},
  pages     = {625--632}
}

@article{nie_quasioracle_2017,
  title    = {Quasi-Oracle Estimation of Heterogeneous Treatment Effects},
  abstract = {Flexible estimation of heterogeneous treatment effects lies at the heart of many statistical challenges, such as personalized medicine and optimal resource allocation. In this paper, we develop a general class of two-step algorithms for heterogeneous treatment effect estimation in observational studies. We first estimate marginal effects and treatment propensities in order to form an objective function that isolates the causal component of the signal. Then, we optimize this data-adaptive objective function. Our approach has several advantages over existing methods. From a practical perspective, our method is flexible and easy to use: In both steps, we can use any loss-minimization method, e.g., penalized regression, deep neural networks, or boosting; moreover, these methods can be fine-tuned by cross validation. Meanwhile, in the case of penalized kernel regression, we show that our method has a quasi-oracle property: Even if the pilot estimates for marginal effects and treatment propensities are not particularly accurate, we achieve the same error bounds as an oracle who has a priori knowledge of these two nuisance components. We implement variants of our approach based on penalized regression, kernel ridge regression, and boosting in a variety of simulation setups, and find promising performance relative to existing baselines.},
  journal  = {Biometrika},
  volume   = {108},
  number   = {2},
  pages    = {299--319},
  author   = {Nie, Xinkun and Wager, Stefan},
  date     = {2017-12-13},
  keywords = {Statistics - Machine Learning, Economics - Econometrics, Mathematics - Statistics Theory}
}

@book{niswander_women_1972,
  title      = {The Women and Their Pregnancies: The Collaborative Perinatal Study of the National Institute of Neurological Diseases and Stroke},
  shorttitle = {The Women and Their Pregnancies},
  pagetotal  = {562},
  publisher  = {National Institute of Health},
  author     = {Niswander, Kenneth R. and Stroke, United States National Institute of Neurologiacal Diseases and},
  date       = {1972},
  note       = {Google-Books-{ID}: A0bdVhlhDQkC}
}
@article{orjones_streptomycin_1948,
  title   = {Streptomycin {Treatment} of {Pulmonary} {Tuberculosis}},
  volume  = {2},
  number  = {4582},
  journal = {British Medical Journal},
  author  = {{OR, Jones} and {WD,Platt}},
  month   = oct,
  year    = {1948},
  pages   = {769--782}
}
@article{pedregosa_scikitlearn_2011,
  title      = {Scikit-learn: Machine Learning in Python},
  volume     = {12},
  shorttitle = {Scikit-learn},
  abstract   = {Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and {API} consistency. It has minimal dependencies and is distributed under the simplified {BSD} license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from http://scikit-learn.sourceforge.net.},
  pages      = {2825--2830},
  number     = {85},
  journal    = {Journal of Machine Learning Research},
  author     = {Pedregosa, Fabian and Varoquaux, Gaël and Gramfort,
                Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier
                and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg,
                Vincent and Vanderplas, Jake and Passos, Alexandre and Cournapeau, David
                and Brucher, Matthieu and Perrot, Matthieu and Duchesnay, \'Edouard},
  date       = {2011}
}
@article{perez2022beyond,
  title   = {Beyond calibration: estimating the grouping loss of modern neural networks},
  author  = {Perez-Lebel, Alexandre and Morvan, Marine Le and Varoquaux, Ga{\"e}l},
  journal = {arXiv preprint arXiv:2210.16315},
  year    = {2022}
}
@article{perperoglou_review_2019,
  title        = {A review of spline function procedures in R},
  volume       = {19},
  abstract     = {With progress on both the theoretical and the computational fronts the use of spline modelling has become an established tool in statistical regression analysis. An important issue in spline modelling is the availability of user friendly, well documented software packages. Following the idea of the {STRengthening} Analytical Thinking for Observational Studies initiative to provide users with guidance documents on the application of statistical methods in observational research, the aim of this article is to provide an overview of the most widely used spline-based techniques and their implementation in R.},
  pages        = {46},
  number       = {1},
  journaltitle = {{BMC} Medical Research Methodology},
  shortjournal = {{BMC} Medical Research Methodology},
  author       = {Perperoglou, Aris and Sauerbrei, Willi and Abrahamowicz, Michal and Schmid, Matthias},
  date         = {2019-03-06}
}
@article{platt_probabilistic_1999,
  title   = {Probabilistic {Outputs} for {Support} {Vector} {Machines} and {Comparisons} to {Regularized} {Likelihood} {Methods}},
  journal = {Advances in Large Margin Classifiers},
  author  = {Platt, John C. and Platt, John C.},
  year    = {1999},
  pages   = {61--74}
}

@article{poldrack2020establishment,
  title     = {Establishment of best practices for evidence for prediction: a review},
  author    = {Poldrack, Russell A and Huckins, Grace and Varoquaux, Gael},
  journal   = {JAMA psychiatry},
  volume    = {77},
  number    = {5},
  pages     = {534--540},
  year      = {2020},
  publisher = {American Medical Association}
}

@article{powers_methods_2018,
  title    = {Some methods for heterogeneous treatment effect estimation in high dimensions},
  volume   = {37},
  abstract = {When devising a course of treatment for a patient, doctors often have little quantitative evidence on which to base their decisions, beyond their medical education and published clinical trials. Stanford Health Care alone has millions of electronic medical records that are only just recently being leveraged to inform better treatment recommendations. These data present a unique challenge because they are high dimensional and observational. Our goal is to make personalized treatment recommendations based on the outcomes for past patients similar to a new patient. We propose and analyze 3 methods for estimating heterogeneous treatment effects using observational data. Our methods perform well in simulations using a wide variety of treatment effect functions, and we present results of applying the 2 most promising methods to data from The {SPRINT} Data Analysis Challenge, from a large randomized trial of a treatment for high blood pressure.},
  pages    = {1767--1787},
  number   = {11},
  journal  = {Statistics in Medicine},
  author   = {Powers, Scott and Qian, Junyang and Jung, Kenneth and Schuler, Alejandro and Shah, Nigam H. and Hastie, Trevor and Tibshirani, Robert},
  date     = {2018},
  keywords = {machine learning, causal inference, personalized medicine}
}

@article{radley2006off,
  title     = {Off-label prescribing among office-based physicians},
  author    = {Radley, David C and Finkelstein, Stan N and Stafford, Randall S},
  journal   = {Archives of internal medicine},
  volume    = {166},
  number    = {9},
  pages     = {1021--1026},
  year      = {2006},
  publisher = {American Medical Association}
}
@inproceedings{rahimi_random_2008,
  title     = {Random Features for Large-Scale Kernel Machines},
  volume    = {20},
  booktitle = {Advances in Neural Information Processing Systems},
  author    = {Rahimi, Ali and Recht, Benjamin},
  date      = {2008}
}
@article{rahimian2018predicting,
  title     = {Predicting the risk of emergency admission with machine learning: Development and validation using linked electronic health records},
  author    = {Rahimian, Fatemeh and Salimi-Khorshidi, Gholamreza and Payberah, Amir H and Tran, Jenny and Ayala Solares, Roberto and Raimondi, Francesca and Nazarzadeh, Milad and Canoy, Dexter and Rahimi, Kazem},
  journal   = {PLoS medicine},
  volume    = {15},
  number    = {11},
  pages     = {e1002695},
  year      = {2018},
  publisher = {Public Library of Science San Francisco, CA USA}
}

@article{rajkomar2019machine,
  title     = {Machine learning in medicine},
  author    = {Rajkomar, Alvin and Dean, Jeffrey and Kohane, Isaac},
  journal   = {New England Journal of Medicine},
  volume    = {380},
  number    = {14},
  pages     = {1347--1358},
  year      = {2019},
  publisher = {Mass Medical Soc}
}

@article{robins_new_1986,
  title        = {A new approach to causal inference in mortality studies with a sustained exposure period—application to control of the healthy worker survivor effect},
  volume       = {7},
  abstract     = {In observational cohort mortality studies with prolonged periods of exposure to the agent under study, it is not uncommon for risk factors for death to be determinants of subsequent exposure. For instance, in occupational mortality studies date of termination of employment is both a determinant of future exposure (since terminated individuals receive no further exposure) and an independent risk factor for death (since disabled individuals tend to leave employment). When current risk factor status determines subsequent exposure and is determined by previous exposure, standard analyses that estimate age-specific mortality rates as a function of cumulative exposure may underestimate the true effect of exposure on mortality whether or not one adjusts for the risk factor in the analysis. This observation raises the question, which if any population parameters can be given a causal interpretation in observational mortality studies? In answer, we offer a graphical approach to the identification and computation of causal parameters in mortality studies with sustained exposure periods. This approach is shown to be equivalent to an approach in which the observational study is identified with a hypothetical double-blind randomized trial in which data on each subject's assigned treatment protocol has been erased from the data file. Causal inferences can then be made by comparing mortality as a function of treatment protocol, since, in a double-blind randomized trial missing data on treatment protocol, the association of mortality with treatment protocol can still be estimated. We reanalyze the mortality experience of a cohort of arsenic-exposed copper smelter workers with our method and compare our results with those obtained using standard methods. We find an adverse effect of arsenic exposure on all-cause and lung cancer mortality which standard methods fail to detect.},
  pages        = {1393--1512},
  number       = {9},
  journal      = {Mathematical Modelling},
  shortjournal = {Mathematical Modelling},
  author       = {Robins, James},
  date         = {1986-01-01}
}

@article{robins_role_1986,
  title   = {The role of model selection in causal inference from non experimental data},
  volume  = {123},
  pages   = {392--402},
  number  = {3},
  journal = {American Journal of Epidemiology},
  author  = {Robins, James M. and Greenland, Sander},
  date    = {1986-03}
}


@article{robinson_rootnconsistent_1988,
  title    = {Root-N-Consistent Semiparametric Regression},
  abstract = {One type of semiparametric regression on an {\textless}tex-math{\textgreater}\${\textbackslash}scr\{R\}{\textasciicircum}\{p\}{\textbackslash}times {\textbackslash}scr\{R\}{\textasciicircum}\{q\}{\textbackslash}text\{-valued\}\${\textless}/tex-math{\textgreater} random variable (X, Z) is β′X + θ(Z), where β and θ(Z) are an unknown slope coefficient vector and function, and X is neither wholly dependent on Z nor necessarily independent of it. Estimators of β based on incorrect parameterization of θ are generally inconsistent, whereas consistent nonparametric estimators deviate from β by a larger probability order than N$^{\textrm{-1/2}}$, where N is sample size. An estimator generalizing the ordinary least squares estimator of β is constructed by inserting nonparametric regression estimators in the nonlinear orthogonal projection on Z. Under regularity conditions β̂ is shown to be {\textless}tex-math{\textgreater}\$N{\textasciicircum}\{1/2\}{\textbackslash}text\{-consistent\}\${\textless}/tex-math{\textgreater} for β and asymptotically normal, and a consistent estimator of its limiting covariance matrix is given, affording statistical inference that is not only asymptotically valid but has nonzero asymptotic first-order efficiency relative to estimators based on a correctly parameterized θ. We discuss the identification problem and β̂'s efficiency, and report results of a Monte Carlo study of finite-sample performance. While the paper focuses on the simplest interesting setting of multiple regression with independent observations, extensions to other econometric models are described, in particular seemingly unrelated and nonlinear regressions, simultaneous equations, distributed lags, and sample selectivity models.},
  pages    = {931--954},
  number   = {4},
  journal  = {Econometrica},
  author   = {Robinson, P. M.},
  date     = {1988}
}
@article{rolling_model_2014,
  title        = {Model selection for estimating treatment effects},
  volume       = {76},
  abstract     = {Researchers often believe that a treatment's effect on a response may be heteroge neous with respect to certain baseline covariates. This is an important premise of personalized medicine. Several methods for estimating heterogeneous treatment effects have been proposed. However, little attention has been given to the problem of choosing between estimators of treat ment effects. Models that best estimate the regression function may not be best for estimating the effect of a treatment; therefore, there is a need for model selection methods that are targeted to treatment effect estimation. We demonstrate an application of the focused information crite rion in this setting and develop a treatment effect cross-validation aimed at minimizing treatment effect estimation errors. Theoretically, treatment effect cross-validation has a model selection consistency property when the data splitting ratio is properly chosen. Practically, treatment effect cross-validation has the flexibility to compare different types of models. We illustrate the meth ods by using simulation studies and data from a clinical trial comparing treatments of patients with human immunodeficiency virus.},
  pages        = {749--769},
  number       = {4},
  journaltitle = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  shortjournal = {J. R. Stat. Soc. B},
  author       = {Rolling, Craig A. and Yang, Yuhong},
  date         = {2014-09}
}

@article{rosenbaum_central_1983,
  title    = {The central role of the propensity score in observational studies for causal effects},
  abstract = {The propensity score is the conditional probability of assignment to a particular treatment given a vector of observed covariates. Both large and small sample theory show that adjustment for the scalar propensity score is sufficient to remove bias due to all observed covariates. Applications include: (i) matched sampling on the univariate propensity score, which is a generalization of discriminant matching, (ii) multivariate adjustment by subclassification on the propensity score where the same subclasses are used to estimate treatment effects for all outcome variables and in all subpopulations, and (iii) visual representation of multivariate covariance adjustment by a twodimensional plot.},
  volume   = {70},
  pages    = {41--55},
  author   = {Rosenbaum, Paul R and Rubin, Donald B},
  date     = {1983},
  journal  = {Biometrika}
}

@article{rothman2008case,
  title     = {Case-control studies, chapter 8},
  author    = {Rothman, KJ and Greenland, S and Lash, TL},
  journal   = {Modern epidemiology},
  pages     = {111--127},
  year      = {2008},
  publisher = {Lippincott Williams \& Wilkins Location Philadelphia, PA}
}

@article{rubin_causal_2005,
  title        = {Causal Inference Using Potential Outcomes},
  volume       = {100},
  abstract     = {Causal effects are defined as comparisons of potential outcomes under different treatments on a common set of units. Observed values of the potential outcomes are revealed by the assignment mechanism—a probabilistic model for the treatment each unit receives as a function of covariates and potential outcomes. Fisher made tremendous contributions to causal inference through his work on the design of randomized experiments, but the potential outcomes perspective applies to other complex experiments and nonrandomized studies as well. As noted by Kempthorne in his 1976 discussion of Savage's Fisher lecture, Fisher never bridged his work on experimental design and his work on parametric modeling, a bridge that appears nearly automatic with an appropriate view of the potential outcomes framework, where the potential outcomes and covariates are given a Bayesian distribution to complete the model specification. Also, this framework crisply separates scientific inference for causal effects and decisions based on such inference, a distinction evident in Fisher's discussion of tests of significance versus tests in an accept/reject framework. But Fisher never used the potential outcomes framework, originally proposed by Neyman in the context of randomized experiments, and as a result he provided generally flawed advice concerning the use of the analysis of covariance to adjust for posttreatment concomitants in randomized trials.},
  pages        = {322--331},
  number       = {469},
  journal      = {Journal of the American Statistical Association},
  shortjournal = {null},
  author       = {Rubin, Donald B},
  date         = {2005-03-01}
}

@inproceedings{saito_counterfactual_2020,
  title        = {Counterfactual Cross-Validation:  Stable Model Selection Procedure for Causal Inference Models},
  abstract     = {We study the model selection problem in conditional average treatment effect ({CATE}) prediction. Unlike previous works on this topic, we focus on preserving the rank order of the performance of candidate {CATE} predictors to enable accurate and stable model selection. To this end, we analyze the model performance ranking problem and formulate guidelines to obtain a better evaluation metric. We then propose a novel metric that can identify the ranking of the performance of {CATE} predictors with high conﬁdence. Empirical evaluations demonstrate that our metric outperforms existing metrics in both model selection and hyperparameter tuning tasks.},
  author       = {Saito, Yuta and Yasui, Shota},
  date         = {2020},
  booktitle    = {International Conference on Machine Learning},
  pages        = {8398--8407},
  year         = {2020},
  organization = {PMLR}
}

@article{schulam_reliable_2017,
  title    = {Reliable Decision Support using Counterfactual Models},
  abstract = {Decision-makers are faced with the challenge of estimating what is likely to happen when they take an action. For instance, if I choose not to treat this patient, are they likely to die? Practitioners commonly use supervised learning algorithms to ﬁt predictive models that help decision-makers reason about likely future outcomes, but we show that this approach is unreliable, and sometimes even dangerous. The key issue is that supervised learning algorithms are highly sensitive to the policy used to choose actions in the training data, which causes the model to capture relationships that do not generalize. We propose using a different learning objective that predicts counterfactuals instead of predicting outcomes under an existing action policy as in supervised learning. To support decision-making in temporal settings, we introduce the Counterfactual Gaussian Process ({CGP}) to predict the counterfactual future progression of continuous-time trajectories under sequences of future actions. We demonstrate the beneﬁts of the {CGP} on two important decision-support tasks: risk prediction and “what if?” reasoning for individualized treatment planning.},
  author   = {Schulam, Peter and Saria, Suchi},
  journal  = {Advances in neural information processing systems},
  volume   = {30},
  date     = {2017-03-30},
  keywords = {Statistics - Machine Learning, Computer Science - Machine Learning, Computer Science - Artificial Intelligence}
}


@article{schuler_comparison_2018,
  title    = {A comparison of methods for model selection when estimating individual treatment effects},
  abstract = {Practitioners in medicine, business, political science, and other ﬁelds are increasingly aware that decisions should be personalized to each patient, customer, or voter. A given treatment (e.g. a drug or advertisement) should be administered only to those who will respond most positively, and certainly not to those who will be harmed by it. Individual-level treatment eﬀects can be estimated with tools adapted from machine learning, but diﬀerent models can yield contradictory estimates. Unlike risk prediction models, however, treatment eﬀect models cannot be easily evaluated against each other using a held-out test set because the true treatment eﬀect itself is never directly observed. Besides outcome prediction accuracy, several metrics that can leverage held-out data to evaluate treatment eﬀects models have been proposed, but they are not widely used. We provide a didactic framework that elucidates the relationships between the diﬀerent approaches and compare them all using a variety of simulations of both randomized and observational data. Our results show that researchers estimating heterogenous treatment eﬀects need not limit themselves to a single model-ﬁtting algorithm. Instead of relying on a single method, multiple models ﬁt by a diverse set of algorithms should be evaluated against each other using an objective function learned from the validation set. The model minimizing that objective should be used for estimating the individual treatment eﬀect for future individuals.},
  journal  = {{arXiv}:1804.05146 [cs, stat]},
  author   = {Schuler, Alejandro and Baiocchi, Michael and Tibshirani, Robert and Shah, Nigam},
  date     = {2018-06-13},
  keywords = {Statistics - Machine Learning, Computer Science - Machine Learning}
}

@article{schuler_targeted_2017,
  title        = {Targeted Maximum Likelihood Estimation for Causal Inference in Observational Studies},
  volume       = {185},
  abstract     = {Estimation of causal effects using observational data continues to grow in popularity in the epidemiologic literature. While many applications of causal effect estimation use propensity score methods or G-computation, targeted maximum likelihood estimation ({TMLE}) is a well-established alternative method with desirable statistical properties. {TMLE} is a doubly robust maximum-likelihood–based approach that includes a secondary “targeting” step that optimizes the bias-variance tradeoff for the target parameter. Under standard causal assumptions, estimates can be interpreted as causal effects. Because {TMLE} has not been as widely implemented in epidemiologic research, we aim to provide an accessible presentation of {TMLE} for applied researchers. We give step-by-step instructions for using {TMLE} to estimate the average treatment effect in the context of an observational study. We discuss conceptual similarities and differences between {TMLE} and 2 common estimation approaches (G-computation and inverse probability weighting) and present findings on their relative performance using simulated data. Our simulation study compares methods under parametric regression misspecification; our results highlight {TMLE}'s property of double robustness. Additionally, we discuss best practices for {TMLE} implementation, particularly the use of ensembled machine learning algorithms. Our simulation study demonstrates all methods using super learning, highlighting that incorporation of machine learning may outperform parametric regression in observational data settings.},
  pages        = {65--73},
  number       = {1},
  journal      = {American Journal of Epidemiology},
  shortjournal = {American Journal of Epidemiology},
  author       = {Schuler, Megan S. and Rose, Sherri},
  date         = {2017-01-01}
}


@inproceedings{shalit_estimating_2017,
  title        = {Estimating individual treatment effect: generalization bounds and algorithms},
  author       = {Shalit, Uri and Johansson, Fredrik D and Sontag, David},
  booktitle    = {International Conference on Machine Learning},
  pages        = {3076--3085},
  year         = {2017},
  organization = {PMLR}
}

@article{shen2017deep,
  title     = {Deep learning in medical image analysis},
  author    = {Shen, Dinggang and Wu, Guorong and Suk, Heung-Il},
  journal   = {Annual review of biomedical engineering},
  volume    = {19},
  pages     = {221--248},
  year      = {2017},
  publisher = {Annual Reviews}
}

@article{shen2019deep,
  title     = {Deep learning to improve breast cancer detection on screening mammography},
  author    = {Shen, Li and Margolies, Laurie R and Rothstein, Joseph H and Fluder, Eugene and McBride, Russell and Sieh, Weiva},
  journal   = {Scientific reports},
  volume    = {9},
  number    = {1},
  pages     = {12495},
  year      = {2019},
  publisher = {Nature Publishing Group UK London}
}

@article{shen2023rctrep,
  title   = {RCTrep: An R Package for the Validation of Estimates of Average Treatment Effects},
  author  = {Shen, Lingjie and Geleijnse, Gijs and Kaptein, Maurits},
  year    = {2023},
  journal = {Journal of Statistical Software}
}


@article{shimoni_benchmarking_2018,
  title    = {Benchmarking Framework for Performance-Evaluation of Causal Inference Analysis},
  abstract = {Causal inference analysis is the estimation of the effects of actions on outcomes. In the context of healthcare data this means estimating the outcome of counter-factual treatments (i.e. including treatments that were not observed) on a patient's outcome. Compared to classic machine learning methods, evaluation and validation of causal inference analysis is more challenging because ground truth data of counter-factual outcome can never be obtained in any real-world scenario. Here, we present a comprehensive framework for benchmarking algorithms that estimate causal effect. The framework includes unlabeled data for prediction, labeled data for validation, and code for automatic evaluation of algorithm predictions using both established and novel metrics. The data is based on real-world covariates, and the treatment assignments and outcomes are based on simulations, which provides the basis for validation. In this framework we address two questions: one of scaling, and the other of data-censoring. The framework is available as open source code at https://github.com/{IBM}-{HRL}-{MLHLS}/{IBM}-Causal-Inference-Benchmarking-Framework},
  journal  = {{arXiv}:1802.05046 [cs, stat]},
  author   = {Shimoni, Yishai and Yanover, Chen and Karavani, Ehud and Goldschmnidt, Yaara},
  date     = {2018-03-20},
  keywords = {Statistics - Machine Learning, Statistics - Methodology, Computer Science - Machine Learning}
}

@article{simon2018predicting,
  title     = {Predicting suicide attempts and suicide deaths following outpatient visits using electronic health records},
  author    = {Simon, Gregory E and Johnson, Eric and Lawrence, Jean M and Rossom, Rebecca C and Ahmedani, Brian and Lynch, Frances L and Beck, Arne and Waitzfelder, Beth and Ziebell, Rebecca and Penfold, Robert B and others},
  journal   = {American Journal of Psychiatry},
  volume    = {175},
  number    = {10},
  pages     = {951--960},
  year      = {2018},
  publisher = {Am Psychiatric Assoc}
}
@article{snowden_implementation_2011,
  title      = {Implementation of {G}-computation on a simulated data set: demonstration of a causal inference technique},
  volume     = {173},
  shorttitle = {Implementation of {G}-computation on a simulated data set},
  abstract   = {The growing body of work in the epidemiology literature focused on G-computation includes theoretical explanations of the method but very few simulations or examples of application. The small number of G-computation analyses in the epidemiology literature relative to other causal inference approaches may be partially due to a lack of didactic explanations of the method targeted toward an epidemiology audience. The authors provide a step-by-step demonstration of G-computation that is intended to familiarize the reader with this procedure. The authors simulate a data set and then demonstrate both G-computation and traditional regression to draw connections and illustrate contrasts between their implementation and interpretation relative to the truth of the simulation protocol. A marginal structural model is used for effect estimation in the G-computation example. The authors conclude by answering a series of questions to emphasize the key characteristics of causal inference techniques and the G-computation procedure in particular.},
  language   = {eng},
  number     = {7},
  journal    = {American Journal of Epidemiology},
  author     = {Snowden, Jonathan M. and Rose, Sherri and Mortimer, Kathleen M.},
  month      = apr,
  year       = {2011},
  keywords   = {Causality, Computer Simulation, Confounding Factors, Epidemiologic, Epidemiologic Research Design, Humans, Models, Statistical, Regression Analysis},
  pages      = {731--738}
}


@article{spasic2020clinical,
  title     = {Clinical text data in machine learning: systematic review},
  author    = {Spasic, Irena and Nenadic, Goran and others},
  journal   = {JMIR medical informatics},
  volume    = {8},
  number    = {3},
  pages     = {e17984},
  year      = {2020},
  publisher = {JMIR Publications Inc., Toronto, Canada}
}


@article{sriperumbudur_integral_2009,
  title    = {On integral probability metrics, {\textbackslash}phi-divergences and binary classification},
  abstract = {A class of distance measures on probabilities —the integral probability metrics ({IPMs}) — is addressed: these include the Wasserstein distance, Dudley metric, and Maximum Mean Discrepancy. {IPMs} have thus far mostly been used in more abstract settings, for instance as theoretical tools in mass transportation problems, and in metrizing the weak topology on the set of all Borel probability measures deﬁned on a metric space. Practical applications of {IPMs} are less common, with some exceptions in the kernel machines literature. The present work contributes a number of novel properties of {IPMs}, which should contribute to making {IPMs} more widely used in practice, for instance in areas where φ-divergences are currently popular.},
  journal  = {{arXiv}:0901.2698 [cs, math]},
  author   = {Sriperumbudur, Bharath K. and Fukumizu, Kenji and Gretton, Arthur and Schölkopf, Bernhard and Lanckriet, Gert R. G.},
  date     = {2009-10-12},
  keywords = {Computer Science - Information Theory}
}

@article{su2018random,
  title     = {Random forests of interaction trees for estimating individualized treatment effects in randomized trials},
  author    = {Su, Xiaogang and Pe{\~n}a, Annette T and Liu, Lei and Levine, Richard A},
  journal   = {Statistics in medicine},
  volume    = {37},
  number    = {17},
  pages     = {2547--2560},
  year      = {2018},
  publisher = {Wiley Online Library}
}

@inproceedings{swaminathan_counterfactual_2015,
  title        = {Counterfactual risk minimization: Learning from logged bandit feedback},
  author       = {Swaminathan, Adith and Joachims, Thorsten},
  booktitle    = {International Conference on Machine Learning},
  pages        = {814--823},
  year         = {2015},
  organization = {PMLR}
}

@article{tian2014simple,
  title     = {A simple method for estimating interactions between a treatment and a large number of covariates},
  author    = {Tian, Lu and Alizadeh, Ash A and Gentles, Andrew J and Tibshirani, Robert},
  journal   = {Journal of the American Statistical Association},
  volume    = {109},
  number    = {508},
  pages     = {1517--1532},
  year      = {2014},
  publisher = {Taylor \& Francis}
}

@article{tmle_package_2012,
  title   = {{tmle}: An {R} Package for Targeted Maximum Likelihood
             Estimation},
  author  = {Susan Gruber and Mark J. {van der Laan}},
  journal = {Journal of Statistical Software},
  year    = {2012},
  volume  = {51},
  number  = {13},
  pages   = {1--35}
}

@book{vanderlaan_unified_2003,
  title     = {Unified methods for censored longitudinal data and causality},
  publisher = {Springer Science \& Business Media},
  author    = {van der Laan, Mark J and Laan, {MJ} and Robins, {JM}},
  date      = {2003}
}

@article{vanderweele2019principles,
  title     = {Principles of confounder selection},
  author    = {VanderWeele, Tyler J},
  journal   = {European journal of epidemiology},
  volume    = {34},
  pages     = {211--219},
  year      = {2019},
  publisher = {Springer}
}

@misc{varoquaux2022evaluating,
  title   = {Evaluating machine learning models and their diagnostic value},
  author  = {Varoquaux, Ga{\"e}l and Colliot, Olivier},
  year    = {2022},
  journal = {{Machine Learning for Brain Disorders}}
}

@article{wager_estimation_2018,
  title    = {Estimation and Inference of Heterogeneous Treatment Effects using Random Forests},
  volume   = {113},
  abstract = {Many scientific and engineering challenges—ranging from personalized medicine to customized marketing recommendations—require an understanding of treatment effect heterogeneity. In this article, we develop a nonparametric causal forest for estimating heterogeneous treatment effects that extends Breiman’s widely used random forest algorithm. In the potential outcomes framework with unconfoundedness, we show that causal forests are pointwise consistent for the true treatment effect and have an asymptotically Gaussian and centered sampling distribution. We also discuss a practical method for constructing asymptotic confidence intervals for the true treatment effect that are centered at the causal forest estimates. Our theoretical results rely on a generic Gaussian theory for a large family of random forest algorithms. To our knowledge, this is the first set of results that allows any type of random forest, including classification and regression forests, to be used for provably valid statistical inference. In experiments, we find causal forests to be substantially more powerful than classical methods based on nearest-neighbor matching, especially in the presence of irrelevant covariates.},
  pages    = {1228--1242},
  number   = {523},
  journal  = {Journal of the American Statistical Association},
  author   = {Wager, Stefan and Athey, Susan},
  date     = {2018-07-03},
  keywords = {Adaptive nearest neighbors matching, Asymptotic normality, Potential outcomes, Unconfoundedness}
}

@article{wang2020prediction,
  title     = {Prediction of breast cancer distant recurrence using natural language processing and knowledge-guided convolutional neural network},
  author    = {Wang, Hanyin and Li, Yikuan and Khan, Seema A and Luo, Yuan},
  journal   = {Artificial intelligence in medicine},
  volume    = {110},
  pages     = {101977},
  year      = {2020},
  publisher = {Elsevier}
}
@article{wendling_comparing_2018,
  title    = {Comparing methods for estimation of heterogeneous treatment effects using observational data from health care databases},
  abstract = {There is growing interest in using routinely collected data from health care databases to study the safety and effectiveness of therapies in “real-world” conditions, as it can provide complementary evidence to that of randomized controlled trials. Causal inference from health care databases is challenging because the data are typically noisy, high dimensional, and most importantly, observational. It requires methods that can estimate heterogeneous treatment effects while controlling for confounding in high dimensions. Bayesian additive regression trees, causal forests, causal boosting, and causal multivariate adaptive regression splines are off-the-shelf methods that have shown good performance for estimation of heterogeneous treatment effects in observational studies of continuous outcomes. However, it is not clear how these methods would perform in health care database studies where outcomes are often binary and rare and data structures are complex. In this study, we evaluate these methods in simulation studies that recapitulate key characteristics of comparative effectiveness studies. We focus on the conditional average effect of a binary treatment on a binary outcome using the conditional risk difference as an estimand. To emulate health care database studies, we propose a simulation design where real covariate and treatment assignment data are used and only outcomes are simulated based on nonparametric models of the real outcomes. We apply this design to 4 published observational studies that used records from 2 major health care databases in the United States. Our results suggest that Bayesian additive regression trees and causal boosting consistently provide low bias in conditional risk difference estimates in the context of health care database studies.},
  pages    = {3309--3324},
  number   = {23},
  journal  = {Statistics in Medicine},
  author   = {Wendling, T. and Jung, K. and Callahan, A. and Schuler, A. and Shah, N. H. and Gallego, B.},
  date     = {2018},
  keywords = {machine learning, simulation, propensity score, health care databases, heterogeneous treatment effects}
}

@article{yala2019deep,
  title     = {A deep learning mammography-based model for improved breast cancer risk prediction},
  author    = {Yala, Adam and Lehman, Constance and Schuster, Tal and Portnoi, Tally and Barzilay, Regina},
  journal   = {Radiology},
  volume    = {292},
  number    = {1},
  pages     = {60--66},
  year      = {2019},
  publisher = {Radiological Society of North America}
}

@article{yurkovich2015systematic,
  title     = {A systematic review identifies valid comorbidity indices derived from administrative health data},
  author    = {Yurkovich, Marko and Avina-Zubieta, J Antonio and Thomas, Jamie and Gorenchtein, Mike and Lacaille, Diane},
  journal   = {Journal of clinical epidemiology},
  volume    = {68},
  number    = {1},
  pages     = {3--14},
  year      = {2015},
  publisher = {Elsevier}
}
@article{zadrozny_obtaining_2001,
  title    = {Obtaining calibrated probability estimates from decision trees and naive {Bayesian} classiﬁers},
  abstract = {Accurate, well-calibrated estimates of class membership probabilities are needed in many supervised learning applications, in particular when a cost-sensitive decision must be made about examples with example-dependent costs. This paper presents simple but successful methods for obtaining calibrated probability estimates from decision tree and naive Bayesian classiﬁers. Using the large and challenging KDD’98 contest dataset as a testbed, we report the results of a detailed experimental comparison of ten methods, according to four evaluation measures. We conclude that binning succeeds in signiﬁcantly improving naive Bayesian probability estimates, while for improving decision tree probability estimates, we recommend smoothing by -estimation and a new variant of pruning that we call curtailment.},
  author   = {Zadrozny, Bianca and Elkan, Charles},
  date     = {2001},
  pages    = {8}
}

@article{zhang2019radiological,
  title     = {Radiological images and machine learning: trends, perspectives, and prospects},
  author    = {Zhang, Zhenwei and Sejdi{\'c}, Ervin},
  journal   = {Computers in biology and medicine},
  volume    = {108},
  pages     = {354--370},
  year      = {2019},
  publisher = {Elsevier}
}

@article{zivich2021machine,
  title     = {Machine learning for causal inference: on the use of cross-fit estimators},
  author    = {Zivich, Paul N and Breskin, Alexander},
  journal   = {Epidemiology (Cambridge, Mass.)},
  volume    = {32},
  number    = {3},
  pages     = {393},
  year      = {2021},
  publisher = {NIH Public Access}
}
