\relax 
\providecommand\zref@newlabel[2]{}
\providecommand\hyper@newdestlabel[2]{}
\providecommand*\new@tpo@label[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\bibstyle{unsrtnat}
\citation{moons2009prognosis,steyerberg2019clinical}
\citation{beam2018big,rajkomar2019machine}
\citation{khojaste2022deep,zhang2019radiological,yala2019deep,shen2019deep,nassif2022breast}
\citation{mooney2018bigdata,desai2020comparison,simon2018predicting}
\citation{horng2017creating,wang2020prediction,spasic2020clinical}
\citation{altman2009prognosis,poldrack2020establishment,varoquaux2022evaluating}
\citation{wyatt1995commentary}
\citation{fontana2019can}
\citation{snowden_implementation_2011,sperrin2019explicit,blakely2020reflection}
\citation{hernan_causal_2020,vanderweele2019principles}
\citation{robins_role_1986}
\citation{snowden_implementation_2011}
\citation{wendling_comparing_2018}
\citation{rosenbaum_central_1983,austin_moving_2015,casucci2018estimating,grose_use_2020}
\citation{su2018random,lamont2018identification,hoogland2021tutorial}
\citation{hill_bayesian_2011}
\citation{laan_targeted_2011,schuler_targeted_2017}
\citation{powers_methods_2018}
\citation{powers_methods_2018}
\citation{wager_estimation_2018,athey_generalized_2019}
\citation{kunzel_metalearners_2019}
\citation{nie_quasioracle_2017}
\citation{chernozhukov_double_2018}
\citation{fang2019applying,dorie_automated_2019}
\@writefile{toc}{\contentsline {section}{Introduction}{1}{section*.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Extending prediction to prescription needs causality}{1}{subsection*.3}\protected@file@percent }
\citation{schuler_comparison_2018,alaa_validating_2019}
\citation{poldrack2020establishment,varoquaux2022evaluating}
\citation{charlson_new_1987}
\citation{naimi2023defining,imbens_causal_2015}
\citation{rubin_causal_2005}
\@writefile{toc}{\contentsline {paragraph}{Objectives and structure of the paper}{2}{paragraph*.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Illustration: the best predictor may not estimate best causal effects}{2}{subsection*.6}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces The best predictor may not estimate best causal effects}}{2}{figure.caption.5}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:toy_example}{{1}{2}{The best predictor may not estimate best causal effects}{figure.caption.5}{}}
\@writefile{toc}{\contentsline {section}{Methods}{2}{section*.7}\protected@file@percent }
\newlabel{sec:framework}{{}{2}{Methods}{section*.7}{}}
\@writefile{toc}{\contentsline {subsection}{Neyman-Rubin Potential Outcomes framework}{2}{subsection*.8}\protected@file@percent }
\newlabel{sec:neyman_rubin}{{}{2}{Neyman-Rubin Potential Outcomes framework}{subsection*.8}{}}
\@writefile{toc}{\contentsline {paragraph}{Settings}{2}{paragraph*.9}\protected@file@percent }
\citation{robins_new_1986}
\citation{snowden_implementation_2011}
\citation{robinson_rootnconsistent_1988}
\citation{rosenbaum_central_1983}
\citation{chernozhukov_double_2018}
\citation{hill_bayesian_2011}
\citation{schuler_comparison_2018}
\citation{vanderlaan_unified_2003}
\citation{wager_estimation_2018}
\citation{nie_quasioracle_2017}
\citation{nie_quasioracle_2017}
\citation{schuler_comparison_2018}
\citation{schulam_reliable_2017,hill_bayesian_2011}
\@writefile{toc}{\contentsline {paragraph}{Causal assumptions}{3}{paragraph*.10}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Estimating treatment effects with outcome models -- g-computation \cite  {robins_new_1986}}{3}{paragraph*.11}\protected@file@percent }
\newlabel{subsec:estimators}{{}{3}{Estimating treatment effects with outcome models -- g-computation \cite {robins_new_1986}}{paragraph*.11}{}}
\newlabel{eq:mu_identification}{{1}{3}{Estimating treatment effects with outcome models -- g-computation \cite {robins_new_1986}}{equation.0.1}{}}
\newlabel{eq:tau_population}{{2}{3}{Estimating treatment effects with outcome models -- g-computation \cite {robins_new_1986}}{equation.0.2}{}}
\newlabel{eq:ate_estimate}{{3}{3}{Estimating treatment effects with outcome models -- g-computation \cite {robins_new_1986}}{equation.0.3}{}}
\newlabel{eq:cate_estimate}{{4}{3}{Estimating treatment effects with outcome models -- g-computation \cite {robins_new_1986}}{equation.0.4}{}}
\@writefile{toc}{\contentsline {paragraph}{Robinson decomposition}{3}{paragraph*.12}\protected@file@percent }
\newlabel{def:m}{{5}{3}{Robinson decomposition}{equation.0.5}{}}
\newlabel{def:propensity_score}{{6}{3}{Robinson decomposition}{equation.0.6}{}}
\newlabel{eq:r_decomposition}{{7}{3}{Robinson decomposition}{equation.0.7}{}}
\@writefile{toc}{\contentsline {subsection}{Model-selection risks, oracle and feasible}{3}{subsection*.15}\protected@file@percent }
\newlabel{sec:problem:model_selection}{{}{3}{Model-selection risks, oracle and feasible}{subsection*.15}{}}
\@writefile{toc}{\contentsline {paragraph}{Causal model selection}{3}{paragraph*.16}\protected@file@percent }
\newlabel{sec:problem:causal_selection}{{}{3}{Causal model selection}{paragraph*.16}{}}
\newlabel{eq:causal_model_selection}{{8}{3}{Causal model selection}{equation.0.8}{}}
\@writefile{toc}{\contentsline {paragraph}{The $\tau \text  {-risk}$: an oracle error risk}{3}{paragraph*.17}\protected@file@percent }
\newlabel{paragraph:oracle_metrics}{{}{3}{The $\tau \text {-risk}$: an oracle error risk}{paragraph*.17}{}}
\newlabel{eq:tau_risk}{{9}{3}{The $\tau \text {-risk}$: an oracle error risk}{equation.0.9}{}}
\@writefile{toc}{\contentsline {paragraph}{Feasible error risks}{3}{paragraph*.18}\protected@file@percent }
\newlabel{paragraph:feasible_metrics}{{}{3}{Feasible error risks}{paragraph*.18}{}}
\@writefile{toc}{\contentsline {subsection}{Estimation and model selection procedure}{3}{subsection*.19}\protected@file@percent }
\newlabel{problem:estimation_procedure}{{}{3}{Estimation and model selection procedure}{subsection*.19}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Model selection procedure\relax }}{3}{algorithm.1}\protected@file@percent }
\newlabel{problem:estimation_procedure:algo}{{1}{3}{Model selection procedure\relax }{algorithm.1}{}}
\citation{shalit_estimating_2017}
\citation{schuler_comparison_2018,alaa_validating_2019}
\citation{howe_splines_2011,perperoglou_review_2019}
\citation{rahimi_random_2008}
\citation{kunzel_metalearners_2019,shen2023rctrep}
\citation{dorie_automated_2019}
\citation{niswander_women_1972}
\citation{shimoni_benchmarking_2018}
\citation{macdorman_infant_1998}
\citation{louizos_causal_2017}
\citation{almond_costs_2005}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Review of causal risks --- The $R\text  {-risk}^*$ is called $\tau \text  {-risk}_R$ in \cite  {schuler_comparison_2018}. \relax }}{4}{table.caption.14}\protected@file@percent }
\newlabel{tab:evaluation_metrics}{{1}{4}{Review of causal risks --- The $R\text {-risk}^*$ is called $\tau \text {-risk}_R$ in \cite {schuler_comparison_2018}. \relax }{table.caption.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Estimation procedure for causal model selection.\relax }}{4}{figure.caption.20}\protected@file@percent }
\newlabel{problem:estimation_procedure:figure}{{2}{4}{Estimation procedure for causal model selection.\relax }{figure.caption.20}{}}
\@writefile{toc}{\contentsline {subsection}{R-risk as reweighted oracle metric}{4}{subsection*.21}\protected@file@percent }
\newlabel{theory:r_risk_rewrite}{{}{4}{R-risk as reweighted oracle metric}{subsection*.21}{}}
\newlabel{eq:residuals}{{}{4}{R-risk as reweighted oracle metric}{subsection*.21}{}}
\newlabel{theory:prop:r_risk_rewrite}{{1}{4}{$R \text {-risk}$ as reweighted $\tau \text {-risk}$}{theorem.0.1}{}}
\@writefile{toc}{\contentsline {section}{Empirical Study}{4}{section*.22}\protected@file@percent }
\newlabel{sec:empirical_study}{{}{4}{Empirical Study}{section*.22}{}}
\@writefile{toc}{\contentsline {subsection}{Caussim: Extensive simulation settings}{4}{subsection*.23}\protected@file@percent }
\newlabel{subsec:simulations}{{}{4}{Caussim: Extensive simulation settings}{subsection*.23}{}}
\@writefile{toc}{\contentsline {paragraph}{Data Generation}{4}{paragraph*.24}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Family of candidate estimators}{4}{paragraph*.25}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Semi-simulated datasets}{4}{subsection*.26}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Datasets}{4}{paragraph*.27}\protected@file@percent }
\newlabel{semi_simulated:datasets}{{}{4}{Datasets}{paragraph*.27}{}}
\citation{schuler_targeted_2017}
\citation{kendall_new_1938}
\citation{athey2016recursive,gutierrez_causal_2016}
\citation{nie_quasioracle_2017}
\@writefile{toc}{\contentsline {paragraph}{Family of candidate estimators}{5}{paragraph*.28}\protected@file@percent }
\newlabel{semi_simulated:candidate_estimators}{{}{5}{Family of candidate estimators}{paragraph*.28}{}}
\@writefile{toc}{\contentsline {paragraph}{Nuisance estimators}{5}{paragraph*.29}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{Measuring overlap between treated and non treated}{5}{subsection*.30}\protected@file@percent }
\newlabel{subsec:measuring_overlap}{{}{5}{Measuring overlap between treated and non treated}{subsection*.30}{}}
\@writefile{toc}{\contentsline {section}{Results: factors driving good model selection}{5}{section*.32}\protected@file@percent }
\newlabel{empirical_study:results}{{}{5}{Results: factors driving good model selection}{section*.32}{}}
\@writefile{toc}{\contentsline {paragraph}{The $R\text  {-risk}$ is the best metric}{5}{paragraph*.33}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces \textbf  {The $R$-risk is the best metric}: Relative Kendall's $\tau $ agreement with $\tau \text  {-risk}$. Strong and Weak overlap correspond to the first and last tertiles of the overlap distribution measured with Normalized Total Variation eq. \ref  {eq:ntv}. \ref  {apd:experiments:additional_results} presents the same results by adding semi-oracle risks in Figure \ref  {apd:fig:relative_kendalls_all_datasets_all_metrics}, measured with absolute Kendall's in Figure \ref  {apd:fig:all_datasets_tau_risk_ranking_agreement} and with $\tau \mathrm {-risk}$ gains in Figure \ref  {apd:all_datasets_normalized_bias_tau_risk_to_best_method}. Table \ref  {apd:table:relative_kendalls_all_datasets} gives median and IQR of the relative Kendall.\relax }}{5}{figure.caption.31}\protected@file@percent }
\newlabel{fig:relative_kendalls_all_datasets}{{3}{5}{\textbf {The $R$-risk is the best metric}: Relative Kendall's $\tau $ agreement with $\tau \text {-risk}$. Strong and Weak overlap correspond to the first and last tertiles of the overlap distribution measured with Normalized Total Variation eq. \ref {eq:ntv}. \ref {apd:experiments:additional_results} presents the same results by adding semi-oracle risks in Figure \ref {apd:fig:relative_kendalls_all_datasets_all_metrics}, measured with absolute Kendall's in Figure \ref {apd:fig:all_datasets_tau_risk_ranking_agreement} and with $\tau \mathrm {-risk}$ gains in Figure \ref {apd:all_datasets_normalized_bias_tau_risk_to_best_method}. Table \ref {apd:table:relative_kendalls_all_datasets} gives median and IQR of the relative Kendall.\relax }{figure.caption.31}{}}
\@writefile{toc}{\contentsline {paragraph}{Model selection is harder for low population overlap}{5}{paragraph*.34}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces \textbf  {Model selection is harder for low population overlap}: Kendall's $\tau $ agreement with $\tau \text  {-risk}$. Strong, medium and Weak overlap are the tertiles of the overlap measured with NTV eq. \ref  {eq:ntv}. Supplementary materials presents results for all metrics in Figure \ref  {apd:fig:all_datasets_overlap_effect} in absolute Kendall's and continuous overlap values in Figure {\ref  {apd:fig:all_datasets_tau_risk_ranking_agreement}}.\relax }}{5}{figure.caption.35}\protected@file@percent }
\newlabel{fig:all_datasets_overlap_effect_r_risk}{{4}{5}{\textbf {Model selection is harder for low population overlap}: Kendall's $\tau $ agreement with $\tau \text {-risk}$. Strong, medium and Weak overlap are the tertiles of the overlap measured with NTV eq. \ref {eq:ntv}. Supplementary materials presents results for all metrics in Figure \ref {apd:fig:all_datasets_overlap_effect} in absolute Kendall's and continuous overlap values in Figure {\ref {apd:fig:all_datasets_tau_risk_ranking_agreement}}.\relax }{figure.caption.35}{}}
\@writefile{toc}{\contentsline {paragraph}{Nuisances can be estimated on the same data as outcome models}{5}{paragraph*.36}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Stacked models are good overall estimators of nuisances}{5}{paragraph*.38}\protected@file@percent }
\citation{platt_probabilistic_1999,zadrozny_obtaining_2001,niculescu-mizil_predicting_2005,minderer_revisiting_2021}
\citation{perez2022beyond}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces \textbf  {Nuisances can be estimated on the same data as outcome models}: Results for the R-risk are similar between the \leavevmode {\color  {MidnightBlue}shared nuisances/candidate set} and the \leavevmode {\color  {RedOrange}separated nuisances set} procedures. Figure \ref  {apd:fig:procedures_comparison_all_metrics} details results for all metrics.\relax }}{6}{figure.caption.37}\protected@file@percent }
\newlabel{fig:procedures_comparison}{{5}{6}{\textbf {Nuisances can be estimated on the same data as outcome models}: Results for the R-risk are similar between the \textcolor {MidnightBlue}{shared nuisances/candidate set} and the \textcolor {RedOrange}{separated nuisances set} procedures. Figure \ref {apd:fig:procedures_comparison_all_metrics} details results for all metrics.\relax }{figure.caption.37}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces \textbf  {\leavevmode {\color  {DarkOrchid}Stacked models} are good overall estimators of the nuisances}: Results are shown only for the R-risk; Figure \ref  {apd:fig:nuisances_comparison} details every metrics. For Twins, where the true propensity model is linear, \leavevmode {\color  {DarkOrchid}stacked} and \leavevmode {\color  {ForestGreen}linear} estimations of the nuisances performs equivalently, even for a downsampled version (N=4,794). \relax }}{6}{figure.caption.39}\protected@file@percent }
\newlabel{fig:all_datasets_nuisances_comparison}{{6}{6}{\textbf {\textcolor {DarkOrchid}{Stacked models} are good overall estimators of the nuisances}: Results are shown only for the R-risk; Figure \ref {apd:fig:nuisances_comparison} details every metrics. For Twins, where the true propensity model is linear, \textcolor {DarkOrchid}{stacked} and \textcolor {ForestGreen}{linear} estimations of the nuisances performs equivalently, even for a downsampled version (N=4,794). \relax }{figure.caption.39}{}}
\@writefile{toc}{\contentsline {section}{Discussion and conclusion}{6}{section*.40}\protected@file@percent }
\newlabel{sec:discussion}{{}{6}{Discussion and conclusion}{section*.40}{}}
\newlabel{sec:conclusion}{{}{6}{Discussion and conclusion}{section*.40}{}}
\@writefile{toc}{\contentsline {paragraph}{Nuisance models: more gain than pain}{6}{paragraph*.41}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{More $R\text  {-risk}$ to select models driving decisions}{6}{paragraph*.42}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{Availability of source code and requirements}{6}{section*.43}\protected@file@percent }
\citation{bouthillier_accounting_2021}
\citation{dorie_automated_2019}
\citation{dorie_automated_2019}
\citation{pedregosa_scikitlearn_2011}
\citation{powers_methods_2018}
\citation{schuler_comparison_2018}
\citation{nie_quasioracle_2017}
\citation{alaa_validating_2019}
\citation{dorie_automated_2019}
\citation{damour_overlap_2020}
\@writefile{toc}{\contentsline {section}{\numberline {A}Additional files}{7}{appendix.A}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {A.1}Variability of ATE estimation on ACIC 2016}{7}{subsection.A.1}\protected@file@percent }
\newlabel{apd:toy_example:acic_2016_ate_variability}{{A.1}{7}{Variability of ATE estimation on ACIC 2016}{subsection.A.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Hyper-parameters grid used for ACIC 2016 ATE variability\relax }}{7}{table.caption.45}\protected@file@percent }
\newlabel{apd:toy_example:acic_2016_ate_variability:table}{{2}{7}{Hyper-parameters grid used for ACIC 2016 ATE variability\relax }{table.caption.45}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.2}Prior work : model selection for outcome modeling (g-computation)}{7}{subsection.A.2}\protected@file@percent }
\newlabel{apd:prior_work}{{A.2}{7}{Prior work : model selection for outcome modeling (g-computation)}{subsection.A.2}{}}
\@writefile{toc}{\contentsline {paragraph}{Simulation studies of causal model selection}{7}{paragraph*.46}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces \textbf  {Different outcome models lead to different estimation errors on the Average Treatment Effects}, on 77 classic simulations with known true causal effect \cite  {dorie_automated_2019}. The different models are ridge regression and random forests with different hyper-parameters (details \ref  {apd:toy_example:acic_2016_ate_variability}). The different configurations are plotted as a function of increasing difference between treated and untreated population --see \autoref  {subsec:measuring_overlap}. There is no systematic best performer; data-driven model selection is important. \relax }}{7}{figure.caption.44}\protected@file@percent }
\newlabel{fig:acic_2016_ate_heterogeneity}{{7}{7}{\textbf {Different outcome models lead to different estimation errors on the Average Treatment Effects}, on 77 classic simulations with known true causal effect \cite {dorie_automated_2019}. The different models are ridge regression and random forests with different hyper-parameters (details \ref {apd:toy_example:acic_2016_ate_variability}). The different configurations are plotted as a function of increasing difference between treated and untreated population --see \autoref {subsec:measuring_overlap}. There is no systematic best performer; data-driven model selection is important. \relax }{figure.caption.44}{}}
\citation{rolling_model_2014}
\citation{gutierrez_causal_2016}
\citation{saito_counterfactual_2020}
\citation{alaa_validating_2019}
\citation{laan_targeted_2011,schuler_targeted_2017}
\citation{chernozhukov_double_2018}
\citation{johansson2022generalization}
\citation{schuler_comparison_2018}
\citation{rubin_causal_2005}
\citation{rosenbaum_central_1983}
\citation{damour_overlap_2020}
\citation{hernan_causal_2020}
\citation{jesson_identifying_2020}
\citation{shalit_estimating_2017}
\citation{vanderlaan_unified_2003}
\citation{wager_estimation_2018}
\citation{athey2016recursive,gutierrez_causal_2016,wager_estimation_2018}
\citation{kunzel_metalearners_2019,nie_quasioracle_2017}
\@writefile{toc}{\contentsline {paragraph}{Theoretical studies of causal model selection}{8}{paragraph*.47}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Statistical guarantees on causal estimation procedures}{8}{paragraph*.48}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {A.3}Causal assumptions}{8}{subsection.A.3}\protected@file@percent }
\newlabel{apd:causal_assumptions}{{A.3}{8}{Causal assumptions}{subsection.A.3}{}}
\newlabel{assumption:ignorability}{{1}{8}{Unconfoundedness}{assumption.1}{}}
\newlabel{eq:ignorability}{{1}{8}{Unconfoundedness}{assumption.1}{}}
\newlabel{assumption:overlap}{{2}{8}{Overlap, also known as Positivity)}{assumption.2}{}}
\newlabel{eq:overlap}{{2}{8}{Overlap, also known as Positivity)}{assumption.2}{}}
\newlabel{assumption:consistency}{{3}{8}{Consistency}{assumption.3}{}}
\newlabel{eq:consistancy}{{3}{8}{Consistency}{assumption.3}{}}
\newlabel{assumption:generalization}{{4}{8}{Generalization}{assumption.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.4}Definitions of feasible risks}{8}{subsection.A.4}\protected@file@percent }
\newlabel{def:feasible_risks}{{A.4}{8}{Definitions of feasible risks}{subsection.A.4}{}}
\newlabel{def:mu_risk}{{1}{8}{Factual $\mu \text {-risk}$}{definition.1}{}}
\newlabel{eq:mu_risk}{{1}{8}{Factual $\mu \text {-risk}$}{definition.1}{}}
\newlabel{def:mu_ipw_risk}{{2}{8}{$\mu \text {-risk}_{IPW}^{\star }$}{definition.2}{}}
\newlabel{eq:mu_ipw_risk}{{2}{8}{$\mu \text {-risk}_{IPW}^{\star }$}{definition.2}{}}
\newlabel{def:tau_ipw_risk}{{3}{8}{$\tau \text {-risk}^{\star }_{IPW}$}{definition.3}{}}
\citation{nie_quasioracle_2017,schuler_comparison_2018}
\citation{robinson_rootnconsistent_1988}
\citation{schuler_comparison_2018}
\citation{schuler_comparison_2018}
\newlabel{def:u_risk}{{4}{9}{$U\text {-risk}^{\star }$}{definition.4}{}}
\newlabel{def:r_risk}{{5}{9}{$R\text {-risk}^{\star }$}{definition.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.5}Proofs: Links between feasible and oracle risks}{9}{subsection.A.5}\protected@file@percent }
\newlabel{apd:proofs}{{A.5}{9}{Proofs: Links between feasible and oracle risks}{subsection.A.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{Reformulation of the $R\text  {-risk}$ as reweighted $\tau \text  {-risk}$}{9}{subsubsection*.49}\protected@file@percent }
\newlabel{apd:proofs:r_risk_rewrite}{{A.5}{9}{Reformulation of the $R\text {-risk}$ as reweighted $\tau \text {-risk}$}{subsubsection*.49}{}}
\newlabel{apd:proofs:prop:r_risk_rewrite}{{1}{9}{$R\text {-risk}$ as reweighted $\tau \text {-risk}$}{proposition*.1}{}}
\newlabel{apd:eq:r_decomposition}{{11}{9}{$R\text {-risk}$ as reweighted $\tau \text {-risk}$}{equation.A.11}{}}
\@writefile{toc}{\contentsline {subsubsection}{Interesting special cases}{9}{subsubsection*.50}\protected@file@percent }
\newlabel{apd:theory:special_cases}{{A.5}{9}{Interesting special cases}{subsubsection*.50}{}}
\@writefile{toc}{\contentsline {paragraph}{Randomization special case}{9}{paragraph*.51}\protected@file@percent }
\newlabel{remark:rct}{{A.5}{9}{Randomization special case}{paragraph*.51}{}}
\citation{austin_introduction_2011,austin_moving_2015}
\citation{damour_overlap_2020,johansson2022generalization}
\citation{gretton2012kernel}
\citation{shalit_estimating_2017,johansson2022generalization}
\citation{sriperumbudur_integral_2009}
\@writefile{toc}{\contentsline {paragraph}{Oracle Bayes predictor}{10}{paragraph*.52}\protected@file@percent }
\newlabel{remark:bayes_oracle}{{A.5}{10}{Oracle Bayes predictor}{paragraph*.52}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.6}Measuring overlap}{10}{subsection.A.6}\protected@file@percent }
\newlabel{apd:motivation_ntv}{{A.6}{10}{Measuring overlap}{subsection.A.6}{}}
\@writefile{toc}{\contentsline {paragraph}{Motivation of the Normalized Total Variation}{10}{paragraph*.53}\protected@file@percent }
\newlabel{eq:ntv}{{16}{10}{Motivation of the Normalized Total Variation}{equation.A.16}{}}
\@writefile{toc}{\contentsline {paragraph}{Measuring overlap without the oracle propensity scores:}{10}{paragraph*.54}\protected@file@percent }
\citation{dorie_automated_2019}
\citation{rahimi_random_2008}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces a) Without calibration, estimation of NTV is not trivial even for boosting models. b) Calibrated classifiers are able to recover the true Normalized Total Variation for all datasets where it is available.\relax }}{11}{figure.caption.57}\protected@file@percent }
\newlabel{apd:overlap:ntv_approximation}{{8}{11}{a) Without calibration, estimation of NTV is not trivial even for boosting models. b) Calibrated classifiers are able to recover the true Normalized Total Variation for all datasets where it is available.\relax }{figure.caption.57}{}}
\@writefile{toc}{\contentsline {paragraph}{Empirical arguments}{11}{paragraph*.55}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Estimating NTV in practice}{11}{paragraph*.56}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces NTV recovers well the overlap settings described in the ACIC paper \cite  {dorie_automated_2019}\relax }}{12}{figure.caption.58}\protected@file@percent }
\newlabel{apd:overlap:penalized_overlap}{{9}{12}{NTV recovers well the overlap settings described in the ACIC paper \cite {dorie_automated_2019}\relax }{figure.caption.58}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Good correlation between overlap measured as normalized Total Variation and Maximum Mean Discrepancy (200 sampled Caussim datasets)\relax }}{12}{figure.caption.59}\protected@file@percent }
\newlabel{apd:overlap:caussim:mmd_vs_ntv}{{10}{12}{Good correlation between overlap measured as normalized Total Variation and Maximum Mean Discrepancy (200 sampled Caussim datasets)\relax }{figure.caption.59}{}}
\citation{pedregosa_scikitlearn_2011}
\newlabel{apd:caussim:max_ipw_vs_ntv}{{11a}{13}{\textbf {Caussim}\relax }{figure.caption.60}{}}
\newlabel{sub@apd:caussim:max_ipw_vs_ntv}{{a}{13}{\textbf {Caussim}\relax }{figure.caption.60}{}}
\newlabel{apd:acic_2016:ntv_vs_max_ipw}{{11b}{13}{\textbf {ACIC 2016}\relax }{figure.caption.60}{}}
\newlabel{sub@apd:acic_2016:ntv_vs_max_ipw}{{b}{13}{\textbf {ACIC 2016}\relax }{figure.caption.60}{}}
\newlabel{apd:acic_2018:ntv_vs_max_ipw}{{11c}{13}{\textbf {ACIC 2018}\relax }{figure.caption.60}{}}
\newlabel{sub@apd:acic_2018:ntv_vs_max_ipw}{{c}{13}{\textbf {ACIC 2018}\relax }{figure.caption.60}{}}
\newlabel{apd:twins:ntv_vs_max_ipw}{{11d}{13}{\textbf {TWINS}\relax }{figure.caption.60}{}}
\newlabel{sub@apd:twins:ntv_vs_max_ipw}{{d}{13}{\textbf {TWINS}\relax }{figure.caption.60}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Maximal value of Inverse Propensity Weights increases exponentially with the overlap as measure by Normalized Total Variation.\relax }}{13}{figure.caption.60}\protected@file@percent }
\newlabel{apd:ntv_vs_max_ipw}{{11}{13}{Maximal value of Inverse Propensity Weights increases exponentially with the overlap as measure by Normalized Total Variation.\relax }{figure.caption.60}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.7}Experiments}{13}{subsection.A.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Details on the data generation process}{13}{subsubsection*.61}\protected@file@percent }
\newlabel{apd:experiments:generation}{{A.7}{13}{Details on the data generation process}{subsubsection*.61}{}}
\citation{dorie_automated_2019}
\citation{shimoni_benchmarking_2018}
\citation{macdorman_infant_1998}
\citation{louizos_causal_2017}
\citation{almond_costs_2005}
\citation{curth_really_2021}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Example of the simulation setup in the input space with two knots --\emph  {ie.} basis functions. The top panel shows the observations in feature space, while the bottom panel displays the two response surfaces on a 1D cut along the black lines drawn on the top panel.\relax }}{14}{figure.caption.62}\protected@file@percent }
\newlabel{fig:simulation_examples}{{12}{14}{Example of the simulation setup in the input space with two knots --\emph {ie.} basis functions. The top panel shows the observations in feature space, while the bottom panel displays the two response surfaces on a 1D cut along the black lines drawn on the top panel.\relax }{figure.caption.62}{}}
\@writefile{toc}{\contentsline {subsubsection}{ Details on the semi-simulated datasets}{14}{subsubsection*.63}\protected@file@percent }
\newlabel{apd:experiments:datasets}{{A.7}{14}{ Details on the semi-simulated datasets}{subsubsection*.63}{}}
\citation{laan_super_2007}
\citation{pedregosa_scikitlearn_2011}
\citation{swaminathan_counterfactual_2015,ionides_truncated_2008}
\@writefile{toc}{\contentsline {subsubsection}{Model selection procedures}{15}{subsubsection*.64}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Nuisances estimation}{15}{paragraph*.65}\protected@file@percent }
\newlabel{apd:experiments:nuisances_hp}{{A.7}{15}{Nuisances estimation}{paragraph*.65}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Hyper-parameters grid used for nuisance models\relax }}{15}{table.caption.66}\protected@file@percent }
\newlabel{apd:experiments:nuisances_hp_grid}{{3}{15}{Hyper-parameters grid used for nuisance models\relax }{table.caption.66}{}}
\@writefile{toc}{\contentsline {subsubsection}{Additional Results}{15}{subsubsection*.67}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Definition of the Kendall's tau, $\kappa $}{15}{paragraph*.68}\protected@file@percent }
\newlabel{apd:experiments:additional_results}{{A.7}{15}{Definition of the Kendall's tau, $\kappa $}{paragraph*.68}{}}
\newlabel{eq:kendall_tau}{{19}{15}{Definition of the Kendall's tau, $\kappa $}{equation.A.19}{}}
\@writefile{toc}{\contentsline {paragraph}{Values of relative $\kappa (\ell ,\tau \mathrm {{-risk}})$ compared to the mean over all metrics Kendall's as shown in the boxplots of Figure \ref  {fig:relative_kendalls_all_datasets}}{15}{paragraph*.69}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Figure \ref  {apd:fig:relative_kendalls_all_datasets_all_metrics} - Results measured in relative Kendall's for feasible and semi-oracle risks}{15}{paragraph*.71}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Figure \ref  {apd:fig:all_datasets_tau_risk_ranking_agreement} - Results measured in absolute Kendall's}{15}{paragraph*.73}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Figure \ref  {apd:all_datasets_normalized_bias_tau_risk_to_best_method} - Results measured as distance to the oracle tau-risk}{15}{paragraph*.75}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Figure \ref  {apd:fig:procedures_comparison_all_metrics} - Stacked models for the nuisances is more efficient}{15}{paragraph*.77}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Figure \ref  {apd:fig:all_datasets_overlap_effect} Low population overlap hinders model selection for all metrics}{15}{paragraph*.79}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Figure \ref  {apd:fig:nuisances_comparison} - Stacked models for the nuisances is more efficient}{15}{paragraph*.81}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Figure \ref  {apd:fig:nuisances_comparison_twins} - Flexible models are performant in recovering nuisances even in linear setups}{15}{paragraph*.83}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{Selecting different seeds and parameters is crucial to draw conclusions}{15}{paragraph*.85}\protected@file@percent }
\newlabel{apd:results:seed_effect}{{A.7}{15}{Selecting different seeds and parameters is crucial to draw conclusions}{paragraph*.85}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Values of relative $\kappa (\ell ,\tau \mathrm {{-risk}})$ compared to the mean over all metrics Kendall's as shown in the boxplots of Figure \ref  {fig:relative_kendalls_all_datasets}\relax }}{16}{table.caption.70}\protected@file@percent }
\newlabel{apd:table:relative_kendalls_all_datasets}{{4}{16}{Values of relative $\kappa (\ell ,\tau \mathrm {{-risk}})$ compared to the mean over all metrics Kendall's as shown in the boxplots of Figure \ref {fig:relative_kendalls_all_datasets}\relax }{table.caption.70}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces \textbf  {The $R$-risk is the best metric}: Relative Kendall's $\tau $ agreement with $\tau \text  {-risk}$. Strong and Weak overlap correspond to the first and last tertiles of the overlap distribution measured with Normalized Total Variation eq. \ref  {eq:ntv}. \relax }}{17}{figure.caption.72}\protected@file@percent }
\newlabel{apd:fig:relative_kendalls_all_datasets_all_metrics}{{13}{17}{\textbf {The $R$-risk is the best metric}: Relative Kendall's $\tau $ agreement with $\tau \text {-risk}$. Strong and Weak overlap correspond to the first and last tertiles of the overlap distribution measured with Normalized Total Variation eq. \ref {eq:ntv}. \relax }{figure.caption.72}{}}
\newlabel{fig:ranking_agreement_w_tau_risk_caussim}{{14a}{18}{\textbf {Caussim}\relax }{figure.caption.74}{}}
\newlabel{sub@fig:ranking_agreement_w_tau_risk_caussim}{{a}{18}{\textbf {Caussim}\relax }{figure.caption.74}{}}
\newlabel{fig:ranking_agreement_tau_risk_acic_2016}{{14b}{18}{\textbf {ACIC 2016}\relax }{figure.caption.74}{}}
\newlabel{sub@fig:ranking_agreement_tau_risk_acic_2016}{{b}{18}{\textbf {ACIC 2016}\relax }{figure.caption.74}{}}
\newlabel{fig:ranking_agreement_w_tau_risk_acic_2018}{{14c}{18}{\textbf {ACIC 2018}\relax }{figure.caption.74}{}}
\newlabel{sub@fig:ranking_agreement_w_tau_risk_acic_2018}{{c}{18}{\textbf {ACIC 2018}\relax }{figure.caption.74}{}}
\newlabel{fig:ranking_agreement_tau_risk_twins}{{14d}{18}{\textbf {TWINS}\relax }{figure.caption.74}{}}
\newlabel{sub@fig:ranking_agreement_tau_risk_twins}{{d}{18}{\textbf {TWINS}\relax }{figure.caption.74}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Agreement with $\tau \text  {-risk}$ ranking of methods function of overlap violation. The lines represent medians, estimated with a lowess. The transparent bands denote the 5\% and 95\% confidence intervals.\relax }}{18}{figure.caption.74}\protected@file@percent }
\newlabel{apd:fig:all_datasets_tau_risk_ranking_agreement}{{14}{18}{Agreement with $\tau \text {-risk}$ ranking of methods function of overlap violation. The lines represent medians, estimated with a lowess. The transparent bands denote the 5\% and 95\% confidence intervals.\relax }{figure.caption.74}{}}
\newlabel{fig:normalized_bias_tau_risk_to_best_method_caussim}{{15a}{19}{\textbf {Caussim}\relax }{figure.caption.76}{}}
\newlabel{sub@fig:normalized_bias_tau_risk_to_best_method_caussim}{{a}{19}{\textbf {Caussim}\relax }{figure.caption.76}{}}
\newlabel{fig:normalized_bias_tau_risk_to_best_method_acic_2016}{{15b}{19}{\textbf {ACIC 2016}\relax }{figure.caption.76}{}}
\newlabel{sub@fig:normalized_bias_tau_risk_to_best_method_acic_2016}{{b}{19}{\textbf {ACIC 2016}\relax }{figure.caption.76}{}}
\newlabel{fig:normalized_bias_tau_risk_to_best_method_acic_2018}{{15c}{19}{\textbf {ACIC 2018}\relax }{figure.caption.76}{}}
\newlabel{sub@fig:normalized_bias_tau_risk_to_best_method_acic_2018}{{c}{19}{\textbf {ACIC 2018}\relax }{figure.caption.76}{}}
\newlabel{fig:normalized_bias_tau_risk_to_best_method_twins}{{15d}{19}{\textbf {TWINS}\relax }{figure.caption.76}{}}
\newlabel{sub@fig:normalized_bias_tau_risk_to_best_method_twins}{{d}{19}{\textbf {TWINS}\relax }{figure.caption.76}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces Metric performances by normalized tau-risk distance to the best method selected with $\tau \text  {-risk}$. All nuisances are learned with the same estimator stacking gradient boosting and ridge regression. Doted and plain lines corresponds to 60\% lowess quantile estimates. This choice of quantile allows to see better the oracle metrics lines for which outliers with a value of 0 distord the curves.\relax }}{19}{figure.caption.76}\protected@file@percent }
\newlabel{apd:all_datasets_normalized_bias_tau_risk_to_best_method}{{15}{19}{Metric performances by normalized tau-risk distance to the best method selected with $\tau \text {-risk}$. All nuisances are learned with the same estimator stacking gradient boosting and ridge regression. Doted and plain lines corresponds to 60\% lowess quantile estimates. This choice of quantile allows to see better the oracle metrics lines for which outliers with a value of 0 distord the curves.\relax }{figure.caption.76}{}}
\newlabel{fig:experiments:procedures_comparison:caussim}{{16a}{20}{\textbf {Caussim}\relax }{figure.caption.78}{}}
\newlabel{sub@fig:experiments:procedures_comparison:caussim}{{a}{20}{\textbf {Caussim}\relax }{figure.caption.78}{}}
\newlabel{fig:experiments:procedures_comparison:acic_2016}{{16b}{20}{\textbf {ACIC 2016}\relax }{figure.caption.78}{}}
\newlabel{sub@fig:experiments:procedures_comparison:acic_2016}{{b}{20}{\textbf {ACIC 2016}\relax }{figure.caption.78}{}}
\newlabel{fig:experiments:procedures_comparison:twins}{{16c}{20}{\textbf {Twins}\relax }{figure.caption.78}{}}
\newlabel{sub@fig:experiments:procedures_comparison:twins}{{c}{20}{\textbf {Twins}\relax }{figure.caption.78}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces Results are similar between the \leavevmode {\color  {MidnightBlue}Shared nuisances/candidate set} and the \leavevmode {\color  {RedOrange}Separated nuisances set} procedure. The experience has not been run on the full metrics for Caussim due to computation costs.\relax }}{20}{figure.caption.78}\protected@file@percent }
\newlabel{apd:fig:procedures_comparison_all_metrics}{{16}{20}{Results are similar between the \textcolor {MidnightBlue}{Shared nuisances/candidate set} and the \textcolor {RedOrange}{Separated nuisances set} procedure. The experience has not been run on the full metrics for Caussim due to computation costs.\relax }{figure.caption.78}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces \textbf  {Low population overlap hinders causal model selection for all metrics}: Kendall's $\tau $ agreement with $\tau \text  {-risk}$. Strong, medium and Weak overlap correspond to the tertiles of the overlap distribution measured with Normalized Total Varation eq. \ref  {eq:ntv}.\relax }}{21}{figure.caption.80}\protected@file@percent }
\newlabel{apd:fig:all_datasets_overlap_effect}{{17}{21}{\textbf {Low population overlap hinders causal model selection for all metrics}: Kendall's $\tau $ agreement with $\tau \text {-risk}$. Strong, medium and Weak overlap correspond to the tertiles of the overlap distribution measured with Normalized Total Varation eq. \ref {eq:ntv}.\relax }{figure.caption.80}{}}
\newlabel{fig:experiments:nuisance_comparison:twins}{{18a}{21}{\textbf {Twins}\relax }{figure.caption.82}{}}
\newlabel{sub@fig:experiments:nuisance_comparison:twins}{{a}{21}{\textbf {Twins}\relax }{figure.caption.82}{}}
\newlabel{fig:experiments:nuisance_comparison:twins_ds}{{18b}{21}{\textbf {Twins downsampled}\relax }{figure.caption.82}{}}
\newlabel{sub@fig:experiments:nuisance_comparison:twins_ds}{{b}{21}{\textbf {Twins downsampled}\relax }{figure.caption.82}{}}
\newlabel{fig:experiments:nuisance_comparison:caussim}{{18c}{21}{\textbf {Caussim}\relax }{figure.caption.82}{}}
\newlabel{sub@fig:experiments:nuisance_comparison:caussim}{{c}{21}{\textbf {Caussim}\relax }{figure.caption.82}{}}
\newlabel{fig:experiments:nuisance_comparison:acic_2016}{{18d}{21}{\textbf {ACIC 2016}\relax }{figure.caption.82}{}}
\newlabel{sub@fig:experiments:nuisance_comparison:acic_2016}{{d}{21}{\textbf {ACIC 2016}\relax }{figure.caption.82}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces Learning the nuisances with \leavevmode {\color  {DarkOrchid}stacked models} (linear and gradient boosting) is important for successful model selection with R-risk. For Twins dataset, there is no improvement for \leavevmode {\color  {DarkOrchid}stacked models} compared to \leavevmode {\color  {ForestGreen}linear models} because of the linearity of the propensity model.\relax }}{21}{figure.caption.82}\protected@file@percent }
\newlabel{apd:fig:nuisances_comparison}{{18}{21}{Learning the nuisances with \textcolor {DarkOrchid}{stacked models} (linear and gradient boosting) is important for successful model selection with R-risk. For Twins dataset, there is no improvement for \textcolor {DarkOrchid}{stacked models} compared to \textcolor {ForestGreen}{linear models} because of the linearity of the propensity model.\relax }{figure.caption.82}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces \textbf  {Flexible models are performant in recovering nuisances in the downsampled Twins dataset.} The propensity score is linear in this setup, making it particularly challenging for flexible models compared to linear methods.\relax }}{22}{figure.caption.84}\protected@file@percent }
\newlabel{apd:fig:nuisances_comparison_twins}{{19}{22}{\textbf {Flexible models are performant in recovering nuisances in the downsampled Twins dataset.} The propensity score is linear in this setup, making it particularly challenging for flexible models compared to linear methods.\relax }{figure.caption.84}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {20}{\ignorespaces Kendall correlation coefficients for each causal metric. Each (color, shape) pair indicates a different (treatment ratio, seed) of the generation process.\relax }}{22}{figure.caption.86}\protected@file@percent }
\newlabel{apd:results:fig:seed_effect}{{20}{22}{Kendall correlation coefficients for each causal metric. Each (color, shape) pair indicates a different (treatment ratio, seed) of the generation process.\relax }{figure.caption.86}{}}
\citation{nie_quasioracle_2017}
\citation{causalevaluations}
\citation{econml}
\citation{chernozhukov_double_2018}
\citation{loiseau_external_2022}
\citation{gao_assessment_2021}
\citation{econml}
\citation{naimi2021challenges}
\citation{tmle_package_2012}
\citation{schuler_comparison_2018}
\bibdata{biblio}
\bibcite{moons2009prognosis}{{1}{2009}{{Moons et~al.}}{{Moons, Royston, Vergouwe, Grobbee, and Altman}}}
\bibcite{steyerberg2019clinical}{{2}{}{{Steyerberg}}{{}}}
\@writefile{lof}{\contentsline {figure}{\numberline {21}{\ignorespaces \textbf  {a) For CATE, a train/test ratio of 0.9/0.1 appears a good trade-off.} b) For ATE, there is a small signal pointing also to 0.9/0.1 (K=10). for ATE. Experiences on 10 replications of all 78 instances of the ACIC 2016 data.\relax }}{23}{figure.caption.88}\protected@file@percent }
\newlabel{fig:train_test_ratio}{{21}{23}{\textbf {a) For CATE, a train/test ratio of 0.9/0.1 appears a good trade-off.} b) For ATE, there is a small signal pointing also to 0.9/0.1 (K=10). for ATE. Experiences on 10 replications of all 78 instances of the ACIC 2016 data.\relax }{figure.caption.88}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.8}Data split choices}{23}{subsection.A.8}\protected@file@percent }
\newlabel{apd:results:data_split}{{A.8}{23}{Data split choices}{subsection.A.8}{}}
\@writefile{toc}{\contentsline {subsubsection}{Use 90\% of the data to estimate outcome models, 10\% to select them}{23}{subsubsection*.87}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Heterogeneity in practices for data split}{23}{subsubsection*.89}\protected@file@percent }
\newlabel{apd:results:k_fold_choices}{{A.8}{23}{Heterogeneity in practices for data split}{subsubsection*.89}{}}
\@writefile{toc}{\contentsline {section}{\numberline {B}Declaration}{23}{appendix.B}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {B.1}Competing interests}{23}{subsection.B.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {C}Author contributions statement}{23}{appendix.C}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {D}Acknowledgments}{23}{appendix.D}\protected@file@percent }
\bibcite{beam2018big}{{3}{2018}{{Beam and Kohane}}{{}}}
\bibcite{rajkomar2019machine}{{4}{2019}{{Rajkomar et~al.}}{{Rajkomar, Dean, and Kohane}}}
\bibcite{khojaste2022deep}{{5}{2022}{{Khojaste-Sarakhsi et~al.}}{{Khojaste-Sarakhsi, Haghighi, Ghomi, and Marchiori}}}
\bibcite{zhang2019radiological}{{6}{2019}{{Zhang and Sejdi{\'c}}}{{}}}
\bibcite{yala2019deep}{{7}{2019}{{Yala et~al.}}{{Yala, Lehman, Schuster, Portnoi, and Barzilay}}}
\bibcite{shen2019deep}{{8}{2019}{{Shen et~al.}}{{Shen, Margolies, Rothstein, Fluder, McBride, and Sieh}}}
\bibcite{nassif2022breast}{{9}{2022}{{Nassif et~al.}}{{Nassif, Talib, Nasir, Afadar, and Elgendy}}}
\bibcite{mooney2018bigdata}{{10}{2018}{{Mooney and Pejaver}}{{}}}
\bibcite{desai2020comparison}{{11}{2020}{{Desai et~al.}}{{Desai, Wang, Vaduganathan, Evers, and Schneeweiss}}}
\bibcite{simon2018predicting}{{12}{2018}{{Simon et~al.}}{{Simon, Johnson, Lawrence, Rossom, Ahmedani, Lynch, Beck, Waitzfelder, Ziebell, Penfold, et~al.}}}
\bibcite{horng2017creating}{{13}{2017}{{Horng et~al.}}{{Horng, Sontag, Halpern, Jernite, Shapiro, and Nathanson}}}
\bibcite{wang2020prediction}{{14}{2020}{{Wang et~al.}}{{Wang, Li, Khan, and Luo}}}
\bibcite{spasic2020clinical}{{15}{2020}{{Spasic et~al.}}{{Spasic, Nenadic, et~al.}}}
\bibcite{altman2009prognosis}{{16}{2009}{{Altman et~al.}}{{Altman, Vergouwe, Royston, and Moons}}}
\bibcite{poldrack2020establishment}{{17}{2020}{{Poldrack et~al.}}{{Poldrack, Huckins, and Varoquaux}}}
\bibcite{varoquaux2022evaluating}{{18}{2022}{{Varoquaux and Colliot}}{{}}}
\bibcite{wyatt1995commentary}{{19}{1995}{{Wyatt and Altman}}{{}}}
\bibcite{fontana2019can}{{20}{2019}{{Fontana et~al.}}{{Fontana, Lyman, Sarker, Padgett, and MacLean}}}
\bibcite{snowden_implementation_2011}{{21}{2011}{{Snowden et~al.}}{{Snowden, Rose, and Mortimer}}}
\bibcite{sperrin2019explicit}{{22}{2019}{{Sperrin et~al.}}{{Sperrin, Jenkins, Martin, and Peek}}}
\bibcite{blakely2020reflection}{{23}{2020}{{Blakely et~al.}}{{Blakely, Lynch, Simons, Bentley, and Rose}}}
\bibcite{hernan_causal_2020}{{24}{}{{Hernán and Robins}}{{}}}
\bibcite{vanderweele2019principles}{{25}{2019}{{VanderWeele}}{{}}}
\bibcite{robins_role_1986}{{26}{}{{Robins and Greenland}}{{}}}
\bibcite{wendling_comparing_2018}{{27}{}{{Wendling et~al.}}{{Wendling, Jung, Callahan, Schuler, Shah, and Gallego}}}
\bibcite{rosenbaum_central_1983}{{28}{}{{Rosenbaum and Rubin}}{{}}}
\bibcite{austin_moving_2015}{{29}{}{{Austin and Stuart}}{{}}}
\bibcite{casucci2018estimating}{{30}{2018}{{Casucci et~al.}}{{Casucci, Lin, Hewner, and Nikolaev}}}
\bibcite{grose_use_2020}{{31}{}{{Grose et~al.}}{{Grose, Wilson, Barkun, Bertens, Martel, Balaa, and Khalil}}}
\bibcite{su2018random}{{32}{2018}{{Su et~al.}}{{Su, Pe{\~n}a, Liu, and Levine}}}
\bibcite{lamont2018identification}{{33}{2018}{{Lamont et~al.}}{{Lamont, Lyons, Jaki, Stuart, Feaster, Tharmaratnam, Oberski, Ishwaran, Wilson, and Van~Horn}}}
\bibcite{hoogland2021tutorial}{{34}{2021}{{Hoogland et~al.}}{{Hoogland, IntHout, Belias, Rovers, Riley, E.~Harrell~Jr, Moons, Debray, and Reitsma}}}
\bibcite{hill_bayesian_2011}{{35}{}{{Hill}}{{}}}
\bibcite{laan_targeted_2011}{{36}{}{{Laan and Rose}}{{}}}
\bibcite{schuler_targeted_2017}{{37}{}{{Schuler and Rose}}{{}}}
\bibcite{powers_methods_2018}{{38}{}{{Powers et~al.}}{{Powers, Qian, Jung, Schuler, Shah, Hastie, and Tibshirani}}}
\bibcite{wager_estimation_2018}{{39}{}{{Wager and Athey}}{{}}}
\bibcite{athey_generalized_2019}{{40}{}{{Athey et~al.}}{{Athey, Tibshirani, and Wager}}}
\bibcite{kunzel_metalearners_2019}{{41}{}{{Künzel et~al.}}{{Künzel, Sekhon, Bickel, and Yu}}}
\bibcite{nie_quasioracle_2017}{{42}{}{{Nie and Wager}}{{}}}
\bibcite{chernozhukov_double_2018}{{43}{}{{Chernozhukov et~al.}}{{Chernozhukov, Chetverikov, Demirer, Duflo, Hansen, Newey, and Robins}}}
\bibcite{fang2019applying}{{44}{2019}{{Fang et~al.}}{{Fang, Annis, Elston-Lafata, and Cykert}}}
\bibcite{dorie_automated_2019}{{45}{2019}{{Dorie et~al.}}{{Dorie, Hill, Shalit, Scott, and Cervone}}}
\bibcite{schuler_comparison_2018}{{46}{}{{Schuler et~al.}}{{Schuler, Baiocchi, Tibshirani, and Shah}}}
\bibcite{alaa_validating_2019}{{47}{}{{Alaa and Schaar}}{{}}}
\bibcite{charlson_new_1987}{{48}{}{{Charlson et~al.}}{{Charlson, Pompei, Ales, and {MacKenzie}}}}
\bibcite{naimi2023defining}{{49}{2023}{{Naimi and Whitcomb}}{{}}}
\bibcite{imbens_causal_2015}{{50}{}{{Imbens and Rubin}}{{}}}
\bibcite{rubin_causal_2005}{{51}{}{{Rubin}}{{}}}
\bibcite{robins_new_1986}{{52}{}{{Robins}}{{}}}
\bibcite{robinson_rootnconsistent_1988}{{53}{}{{Robinson}}{{}}}
\bibcite{vanderlaan_unified_2003}{{54}{}{{van~der Laan et~al.}}{{van~der Laan, Laan, and Robins}}}
\bibcite{schulam_reliable_2017}{{55}{}{{Schulam and Saria}}{{}}}
\bibcite{shalit_estimating_2017}{{56}{2017}{{Shalit et~al.}}{{Shalit, Johansson, and Sontag}}}
\bibcite{howe_splines_2011}{{57}{}{{Howe et~al.}}{{Howe, Cole, Westreich, Greenland, Napravnik, and Eron}}}
\bibcite{perperoglou_review_2019}{{58}{}{{Perperoglou et~al.}}{{Perperoglou, Sauerbrei, Abrahamowicz, and Schmid}}}
\bibcite{rahimi_random_2008}{{59}{}{{Rahimi and Recht}}{{}}}
\bibcite{shen2023rctrep}{{60}{2023}{{Shen et~al.}}{{Shen, Geleijnse, and Kaptein}}}
\bibcite{niswander_women_1972}{{61}{}{{Niswander and Stroke}}{{}}}
\bibcite{shimoni_benchmarking_2018}{{62}{}{{Shimoni et~al.}}{{Shimoni, Yanover, Karavani, and Goldschmnidt}}}
\bibcite{macdorman_infant_1998}{{63}{1998}{{MacDorman and Atkinson}}{{}}}
\bibcite{louizos_causal_2017}{{64}{}{{Louizos et~al.}}{{Louizos, Shalit, Mooij, Sontag, Zemel, and Welling}}}
\bibcite{almond_costs_2005}{{65}{2005}{{Almond et~al.}}{{Almond, Chay, and Lee}}}
\bibcite{kendall_new_1938}{{66}{1938}{{Kendall}}{{}}}
\bibcite{athey2016recursive}{{67}{2016}{{Athey and Imbens}}{{}}}
\bibcite{gutierrez_causal_2016}{{68}{}{{Gutierrez and Gerardy}}{{}}}
\bibcite{platt_probabilistic_1999}{{69}{1999}{{Platt and Platt}}{{}}}
\bibcite{zadrozny_obtaining_2001}{{70}{}{{Zadrozny and Elkan}}{{}}}
\bibcite{niculescu-mizil_predicting_2005}{{71}{2005}{{Niculescu-Mizil and Caruana}}{{}}}
\bibcite{minderer_revisiting_2021}{{72}{2021}{{Minderer et~al.}}{{Minderer, Djolonga, Romijnders, Hubis, Zhai, Houlsby, Tran, and Lucic}}}
\bibcite{perez2022beyond}{{73}{2022}{{Perez-Lebel et~al.}}{{Perez-Lebel, Morvan, and Varoquaux}}}
\bibcite{bouthillier_accounting_2021}{{74}{}{{Bouthillier et~al.}}{{Bouthillier, Delaunay, Bronzi, Trofimov, Nichyporuk, Szeto, Mohammadi~Sepahvand, Raff, Madan, Voleti, Ebrahimi~Kahou, Michalski, Arbel, Pal, Varoquaux, and Vincent}}}
\bibcite{pedregosa_scikitlearn_2011}{{75}{}{{Pedregosa et~al.}}{{Pedregosa, Varoquaux, Gramfort, Michel, Thirion, Grisel, Blondel, Prettenhofer, Weiss, Dubourg, Vanderplas, Passos, Cournapeau, Brucher, Perrot, and Duchesnay}}}
\bibcite{damour_overlap_2020}{{76}{2021}{{D’Amour et~al.}}{{D’Amour, Ding, Feller, Lei, and Sekhon}}}
\bibcite{rolling_model_2014}{{77}{}{{Rolling and Yang}}{{}}}
\bibcite{saito_counterfactual_2020}{{78}{2020}{{Saito and Yasui}}{{}}}
\bibcite{johansson2022generalization}{{79}{2022}{{Johansson et~al.}}{{Johansson, Shalit, Kallus, and Sontag}}}
\bibcite{jesson_identifying_2020}{{80}{}{{Jesson et~al.}}{{Jesson, Mindermann, Shalit, and Gal}}}
\bibcite{austin_introduction_2011}{{81}{2011}{{Austin}}{{}}}
\bibcite{gretton2012kernel}{{82}{}{{Gretton et~al.}}{{Gretton, Borgwardt, Rasch, Schölkopf, and Smola}}}
\bibcite{sriperumbudur_integral_2009}{{83}{}{{Sriperumbudur et~al.}}{{Sriperumbudur, Fukumizu, Gretton, Schölkopf, and Lanckriet}}}
\bibcite{curth_really_2021}{{84}{}{{Curth et~al.}}{{Curth, Svensson, and Weatherall}}}
\bibcite{laan_super_2007}{{85}{}{{Laan et~al.}}{{Laan, Polley, and Hubbard}}}
\bibcite{swaminathan_counterfactual_2015}{{86}{2015}{{Swaminathan and Joachims}}{{}}}
\bibcite{ionides_truncated_2008}{{87}{}{{Ionides}}{{}}}
\bibcite{causalevaluations}{{88}{2019}{{Shimoni et~al.}}{{Shimoni, Karavani, Ravid, Bak, Ng, Alford, Meade, and Goldschmidt}}}
\bibcite{econml}{{89}{}{{Battocchi et~al.}}{{Battocchi, Dillon, Hei, Lewis, Oka, Oprescu, and Syrgkanis}}}
\bibcite{loiseau_external_2022}{{90}{2022}{{Loiseau et~al.}}{{Loiseau, Trichelair, He, Andreux, Zaslavskiy, Wainrib, and Blum}}}
\bibcite{gao_assessment_2021}{{91}{2021}{{Gao et~al.}}{{Gao, Hastie, and Tibshirani}}}
\bibcite{naimi2021challenges}{{92}{2021}{{Naimi et~al.}}{{Naimi, Mishler, and Kennedy}}}
\bibcite{tmle_package_2012}{{93}{2012}{{Gruber and {van der Laan}}}{{}}}
\newlabel{LastPage}{{}{26}{}{page.26}{}}
\xdef\lastpage@lastpage{26}
\xdef\lastpage@lastpageHy{26}
\gdef \@abspage@last{26}
